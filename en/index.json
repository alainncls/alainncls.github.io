[{"categories":null,"contents":" The context Since the launch of the MetaMask platform allowing the creation and use of \u0026#34;plugins\u0026#34; on the wallet, these \u0026#34;Snaps\u0026#34; must be validated by Consensys. Indeed, it is necessary to verify that they respect the imposed security rules, but also the good development and marketing practices pushed by the platform.\nThis validation being totally centralized and relying only on a company, one can see an opposition to the thesis of decentralization and freedom advocated by the web3 ecosystem.\nThis is how the idea of experimenting with the decentralized distribution of these plugins was born.\nThe platform Snaps Permissionless Distribution (\u0026#34;SPD\u0026#34; for short) is based on a respect for the ethos of decentralization and openness of web3. Any developer should be able to declare and present their snap on the platform, and anyone should be able to install it, but also give their opinion on the quality, security or UX provided by this tool.\nIt is the set of opinions, weighted by the quality of their issuer (an auditor having more weight than a brand new MetaMask user, etc.), which allows to determine a trust score to give to the snap, but also to its developer.\nThis project therefore consists in imagining and implementing a dApp that meets the needs of developers and end users, by displaying all the information around a snap. Part of the platform is based on the reputation score calculation algorithm, based on EigenTrust.\nIt is also necessary to store the large amount of data generated by the platform in a decentralized but off-chain manner, so as not to put the transaction costs on the end users.\nThe technical stack Web3 Hardhdat, Solidity\nFront-end React, Gatsby, TypeScript\nBack-end NestJS, MongoDB\nStorage Ceramic\n","permalink":"https://alainnicolas.fr/en/projects/permissionless-snaps-distribution/","tags":["MetaMask","Snaps","Ethereum","React","NestJS"],"title":"Snaps Permissionless Distribution"},{"categories":null,"contents":" The context The first event of the \u0026#34;Linea Voyage\u0026#34;, a quest where participants had to discover the ecosystem of applications and protocols deployed on Linea, met with major enthusiasm. Added to this is the possibility of an airdrop on the same network. Almost automatically, these circumstances led to the appearance of \u0026#34;sybils\u0026#34;, in other words, multiple Ethereum accounts owned by a single person, to maximize their exposure to a potential airdrop.\nTo guarantee a form of equity in the distribution of rewards (only NFTs so far), Linea wanted to set up a system to limit participation to one account per person.\nFrom a fairly simple use case, many features were imagined, leading to the birth of the \u0026#34;Linea Attestation Registry\u0026#34;, quickly named \u0026#34;Verax\u0026#34;.\nThe platform Verax is a public and shared attestation registry that can be deployed on EVM chains. It can be used by dApps to store public interest data, called \u0026#34;attestations\u0026#34;, which can be easily consulted and composed by anyone interested.\nThe platform is composed of 4 major elements:\nThe smart contracts that store the different information of the platform\nThe subgraphs that index the issued attestations\nThe SDK that facilitates interactions with the smart contracts and the subgraphs\nThe explorer that makes the attestations registered on Verax accessible to everyone\nThe result Less than six months after the start of the project, the balance sheet already far exceeds our expectations. Thanks to the \u0026#34;Linea DeFi Voyage\u0026#34;, which relies on Verax to award the ERC-20 token \u0026#34;LineaXP\u0026#34; only to users who have proven their humanity, Verax has recorded more than 1.7 million attestations, issued by a dozen partners. These attestations have been awarded to more than 500,000 unique addresses, which tends to show that the activity of the sybils has been greatly reduced.\nBut, beyond these very satisfactory figures, it is also necessary to note the enthusiasm of the community for this project, with many dApps and protocols in the process of integrating their systems with the platform. And Verax is no longer confined to Linea, since an instance is already running on Arbitrum, and new ones will soon be deployed on other networks.\nThe technical stack Web3 Forge, Hardhdat\nSDK TypeScript, Node.js, GraphQL\nFront-end Vite, React\nTools TheGraph, Goldsky, Etherscan, Gitbook, npm registry\n","permalink":"https://alainnicolas.fr/en/projects/verax-attestation-registry/","tags":["Solidity","TypeScript","Ethereum","React","Linea","Verax"],"title":"Verax Attestation Registry"},{"categories":null,"contents":" The context As part of the launch of its ZK-EVM type network, Consensys wanted to organize an event to invite as many users as possible to interact with the network. Indeed, it was both about making it known, and testing its ability to support a large number of users.\nBut to bring a large number of testers, they needed a reason to come. That’s why, during the \u0026#34;Testnet Voyage\u0026#34;, Consensys chose to rely on applications around DeFi, but also identity with the ENS protocol, or the Lens protocol for the social aspect.\nThe application This is how the idea of \u0026#34;Lineaster\u0026#34;, a fork of Lenster, now renamed \u0026#34;Hey.xyz\u0026#34;, was born. Since the Lens protocol was only deployed on Polygon, it was first necessary to deploy its technical stack on Linea. We benefited from the work of the Lens team to do this.\nIn parallel, the project consisted in adapting the Lenster web client to the protocols available or not on Linea. This is how we had to find a new NFT platform to display the tokens owned by users, or do without OpenZeppelin Defender, which was not yet deployed on Linea at that time.\nThe result Once online, the application was immediately used in the context of the \u0026#34;Testnet Voyage\u0026#34;, and allowed many users to discover the Lens protocol, and test the Linea network.\nTo limit the impact of bots that could have been used to spam the network, we implemented a simple rule: to create a Lens profile on Linea, you had to have previously created an ENS profile on Linea.\nIn the end, the figures exceeded our hopes, with:\n270,000 ENS domains created (cf. Lineascan)\n195,000 Lens names created (cf. Lineascan)\n191,000 Lineaster profiles created\nAlso, as the Linea quest involved using the protocol, and the web client set up, this generated massive engagement with the various features made available:\n150,000 posts created\n7,000 posts re-shared\n9,000 comments\n90,000 posts collected\n100,000 follows\nThe technical stack Front-end React\nWeb3 Lens Protocol, Infura, Linea, Solidity\nTools GitHub\n","permalink":"https://alainnicolas.fr/en/projects/lineaster/","tags":["TypeScript","React","Ethereum","Lens","Linea","Solidity","Blockchain","GitHub","IntelliJ IDEA","TypeScript","pnpm","Kanban","Hardhat"],"title":"Lineaster"},{"categories":null,"contents":" The client For confidentiality reasons, the name of this client cannot be revealed. They are a company already well established in the web3 domain, looking to expand their reach.\nThe project The project involves the creation of a centralized cryptocurrency lending platform, against collateral provided by users.\nTherefore, it was necessary to create a user interface that is understandable and clear enough to attract the largest number, but also to provide all the standard features in this type of application. This interface also comes in an \u0026#34;administrator\u0026#34; version to set the cryptocurrencies available as collateral or to borrow, the current loan offers, etc.\nAs the platform is centralized, it does not rely on smart contracts, but on a backend capable of managing loans, user profiles, etc.\nThe technical stack Front-end React\nBack-end TypeScript (NestJS)\nDatabase PostgreSQL\nWeb3 Infura, Bitcoin, Ethereum, ERC-20, Consensys Codefi Orchestrate\nTools GitHub, GitHub Actions, SonarQube, Docker, Kubernetes\n","permalink":"https://alainnicolas.fr/en/projects/crypto-lending/","tags":["TypeScript","NestJS","React","Docker","PostgreSQL","Bitcoin","Ethereum","ERC-20","Infura","Blockchain","GitHub","IntelliJ IDEA","TypeScript","Jira","Yarn","Scrum"],"title":"Crypto-lending Platform"},{"categories":null,"contents":" The context Creation of a blog for my partner, RoxTheCasbah, to share her passion for knitting and sewing.\nThe technical solution The first version The first version of the site used Ghost, a complete blogging solution competing with WordPress. Based on an open-source and free tool, the blog was hosted on a small AWS machine.\nThe second version In January 2021, I chose to switch to a static site generator, Hugo, in order to be able to host the generated pages for free.\nThe blog is thus deployed and hosted thanks to the free service GitHub Pages.\n","permalink":"https://alainnicolas.fr/en/projects/roxthecasbah/","tags":["Perso","Hugo","Insights","IntelliJ IDEA"],"title":"RoxTheCasbah"},{"categories":null,"contents":" Context Creation of this site to introduce myself and share my passions.\nMy first desire was to showcase my skills in a more controlled way than, for example, on LinkedIn. Similarly, in order to efficiently centralize the articles I have written on Medium or Talan Labs\u0026#39; blog, I needed a blog.\nTechnical Solution The Tool This site is designed using a static site generator, Hugo, and hosted for free thanks to \u0026#34;GitHub Pages\u0026#34; service.\nThe Theme Starting from a blank page is neither easy nor particularly motivating for a personal project, so I chose to start from a theme. Indeed, the Hugo site offers many themes. That’s why I chose the one developed by Eddie Webbinaro: hugo resume.\nHosting This site is hosted using the free GitHub Pages service.\n","permalink":"https://alainnicolas.fr/en/projects/this-site/","tags":["Perso","Hugo","Tarteaucitron","Insights","IntelliJ IDEA","Markdown","AsciiDoc","NPM"],"title":"This Site"},{"categories":null,"contents":" The context Creation of the showcase site for the hair salon \u0026#34;Atelier 58\u0026#34; in Paris.\nThe technical solution Tools This site is designed using a static site generator, Hugo.\nIntegration of the Planity service for making appointments.\nImplementation of navigation statistics tracking via Insights.\nTheme In order to save time, but also to best meet the needs of such a business, I started from the theme \u0026#34;Hugo Hero Theme\u0026#34;.\nHosting This site is hosted via Netlify.\n","permalink":"https://alainnicolas.fr/en/projects/atelier-58/","tags":["Personal","Hugo","Insights","IntelliJ IDEA"],"title":"Atelier 58"},{"categories":null,"contents":" The Client As a major player in the banking sector, BNP Paribas must stay at the forefront of available technologies. This is how blockchain and DLT technologies have become essential.\nLike many large companies, BNP is divided into several entities that can all participate in the same project, which is not without raising challenges around collaboration.\nThe Project To allow all BNP entities to experiment on the blockchain before easily joining a decentralized consortium, it was necessary to make accessible technologies that we can qualify as exotic.\nThis is how our challenge was to make it possible to create entire networks of nodes in a cloud environment with just one click.\nThe Technical Stack Blockchain \u0026amp; DLT The technologies chosen by BNP that we had to make accessible:\nCorda Enterprise\nHyperledger Fabric\nQuorum\nThe Technical Environment Containerization with Docker, many Shell scripts, and installation automation with Ansible.\n","permalink":"https://alainnicolas.fr/en/projects/bnp-blockchain/","tags":["Kanban","Docker","Corda","Quorum","Hyperledger Fabric","Shell","Blockchain","DLT","R3"],"title":"BNP CIB - Blockchain-as-a-Service"},{"categories":null,"contents":" The client Coface is a major player in credit insurance. In order to remain competitive and gain new market share, the company needs to regularly offer new software products and solutions to its customers.\nThe project Optimization and evolution of a web application (Cofanet Policy Master \u0026amp; Cofanet Policy Master Administration) for paying customers.\nTo achieve this, a technical refactoring of the project architecture and a redesign of the major features of the application were needed.\nIn addition to the technical aspects, I had the opportunity to accompany the transition to Agile mode (Scrum) throughout the project, as well as to weigh in on the design decisions to be implemented.\nTechnical stack Client-side JavaScript, HTML5, CSS3, jQuery, Ajax, Bootstrap\nBack-end Spring Service / Spring MVC based on Java 6 before migration to Java 8\nDatabase OracleDB\n","permalink":"https://alainnicolas.fr/en/projects/coface/","tags":["Scrum","Spring MVC","Java","JSP","OracleDB","jQuery","JavaScript"],"title":"Coface"},{"categories":null,"contents":" The client e-Paye develops software dedicated to HRIS: payroll, digital safe, HR management, interview management, etc.\nThe project Among the e-Paye modules, the one allowing the generation of payslips is particularly important and complex. Indeed, it must take into account sometimes changing legislation, but also be able to generate a large number of payslips in a short amount of time.\nTechnical stack Client-side JavaScript, HTML5, CSS3, jQuery, Ajax\nServer-side Spring (Core, MVC, Security, Data)\nJava 8\n","permalink":"https://alainnicolas.fr/en/projects/epaye/","tags":["Scrum","Spring MVC","Java","JSP","mySQL"],"title":"e-Paye"},{"categories":null,"contents":" The client EDF Commerce’s Digital Department has a dedicated team for PoC realization for the group’s various businesses.\nThe mission The mission is to evangelize on the concepts of craftsmanship and industrialize the best practices to be implemented. This is how I was able to initiate the implementation of the first CI/CD pipelines via GitLab and Jenkins, but also push the use of SonarQube for code quality monitoring and Nexus for depositing generated artifacts.\nAs an example, I had the opportunity to create a Spring Boot microservice capable of depositing encrypted files in an on-premises S3 bucket with key retrieval from a Vault, while offering different levels of automated testing.\nTechnical stack Front-end Vue.js (based on Nuxt)\nBack-end Java (Restlet / Spring Boot) and Node.js Hashicorp Vault, AWS SDK (on-premises hosted S3)\nDatabases PostgreSQL, MongoDB\nTools GitLab, Jenkins, SonarQube, Docker, OpenShift\n","permalink":"https://alainnicolas.fr/en/projects/edf/","tags":["Spring Boot","Java","Vue.js","Docker","Spring REST Docs","Vault","GitLab","SonarQube","IntelliJ IDEA"],"title":"EDF Commerce - Craftsmanship Evangelization"},{"categories":null,"contents":" The Client FuturMaster, a software editor specializing in supply chain management, called on Talan Labs to reinforce its development teams and speed up the redesign of its tool suite.\nThe Project That’s how I became part of the first agile squad, made up of 3 developers, 1 designer and 1 Scrum Master. We were tasked with the redesign of the Calibrate Model module, which aims to display the sales history of a product and simulate its future sales.\nTechnical Stack On the Front End React application, with the implementation of a micro-frontend architecture using the Lerna library.\nOn the Back End Spring Boot service based on Java 11, with Spring Data to access data.\nDatabase MongoDB\n","permalink":"https://alainnicolas.fr/en/projects/futurmaster-calibrate-model/","tags":["Scrum","Spring Boot","Java","React","MongoDB","Docker","Spring REST Docs","SonarQube","Azure DevOps","Gradle","IntelliJ IDEA","JavaScript","NPM"],"title":"FuturMaster - Calibrate Model"},{"categories":null,"contents":" Client FuturMaster, a software editor specializing in supply chain management, called upon Talan Labs to strengthen its development teams and accelerate the revamp of its suite of tools.\nProject Following the success of the first Talan Labs squad’s intervention at FuturMaster on the Calibrate Model project, we embarked on the revamp of the Master Data module, which centralizes the data repository that feeds the other software suite modules.\nIn parallel with the development of functional user stories, there are many technical topics to consider, such as a reflection on the future database best suited to the specifics of the objects to be manipulated.\nTechnical Stack Front-end React application, split into micro-frontends. Use of the \u0026#34;hooks\u0026#34; introduced in version 16.8 of React.\nBack-end Spring Boot service based on Java 11, with Spring Data for data access.\nDatabase OracleDB\n","permalink":"https://alainnicolas.fr/en/projects/futurmaster-master-data/","tags":["Scrum","Spring Boot","Java","React","MongoDB","Docker","Spring REST Docs","SonarQube","Azure DevOps","Gradle","IntelliJ IDEA","NPM"],"title":"FuturMaster - Master Data"},{"categories":null,"contents":" The client Gefco is a major player in logistics and in particular in the transportation of automotive parts.\nThe project Development of a scheduling system under constraints.\nTechnical stack Desktop application Java Swing\nDatabase OracleDB\n","permalink":"https://alainnicolas.fr/en/projects/gefco/","tags":["V-model","Java","Java Swing","OracleDB"],"title":"Gefco - Opteam"},{"categories":null,"contents":" Context Share2Gether aims to create an event organization solution, similar to what Meetup.com offers.\nI had the opportunity to share more details about this experience in a dedicated article.\nTechnical Stack Client Side The front-end is a Vue.js application that communicates with smart contracts deployed on the blockchain using the web3.js library.\nBlockchain Side Smart contracts written in Solidity are deployed on an Ethereum blockchain. Standard contracts provided by OpenZeppelin are used.\n","permalink":"https://alainnicolas.fr/en/projects/share2gether/","tags":["Scrum","Blockchain","Ethereum","Vue.js","Node.js","GitLab","IntelliJ IDEA","JavaScript","NPM","Truffle"],"title":"Share2gether"},{"categories":null,"contents":" The Context Like all companies, Talan seeks to catalyze collaboration within itself. In addition, Talan wishes to strengthen its unity as a group despite distinct entities and a presence on 4 continents, and develop a horizontal organization far from classic vertical hierarchies.\nAn internal currency, common to all employees regardless of their location or hierarchical role, makes it possible to find a common point among all the actors in the company. It is also a way to allow everyone to better exchange, including between distinct teams, thus affirming a horizontal hierarchy.\nMore details in the article that I dedicated to Talan Coin.\nThe technical stack Client Side Native iOS application\nNative Android application\nAngular web applications:\nDashboard for statistical tracking and administration\nStore of objects and services payable in Talan Coins\nDesktop client (not put into production)\nServer Side Spring Boot services based on Java 8, serving as an interface between the blockchain and mobile clients.\nBlockchain Ethereum with PoA (Clique) consensus and Solidity smart contracts.\nDatabase MongoDB for storing transactions and accessing them more quickly. Reconstructible from data stored on the blockchain.\n","permalink":"https://alainnicolas.fr/en/projects/talan-talancoin/","tags":["Kanban","Spring Boot","Java","Ethereum","Solidity","MongoDB","Android","iOS","Angular","Docker","Blockchain","GitLab","IntelliJ IDEA","JavaScript","Jira","NPM","Truffle"],"title":"Talan Coin"},{"categories":null,"contents":" Context Maintenance of the Talan Labs blog, writing articles, before a complete overhaul in 2019/2020.\nThe Talan Labs blog allows for sharing the knowledge of Talan Labs employees.\nTechnical Solution Legacy version The first version of the site used WordPress, a well-known solution.\nOverhaul Given the difficulty of keeping such a site up to date while ensuring its security, we chose to radically change the technology to significantly reduce the attack surface.\nBy moving to a static site generator, Hugo, the generated HTML pages are deployed and hosted for free via GitHub Pages.\nTo make the blog GDPR compliant, I had the opportunity to implement the tarteaucitron tool, which manages visitor consent for cookies used by the site (Google Analytics and Disqus).\n","permalink":"https://alainnicolas.fr/en/projects/talan-blog-labs/","tags":["Hugo","WordPress","Tarteaucitron","GitLab","IntelliJ IDEA"],"title":"Talan Labs Blog"},{"categories":null,"contents":" The Context After the success of Talan Coin, the VINCI group wanted to experiment with its own version of the application.\nThis is how we instantiated the VCoin application, with some specific features setting it apart from Talan Coin.\nThe Technical Stack Client Side Native iOS application\nNative Android application\nAngular web applications:\nDashboard for statistical tracking and administration\nStore for objects and services payable in Talan Coins\nDesktop client (not in production)\nBack End Spring Boot services based on Java 8, serving as an interface between the blockchain and mobile clients.\nBlockchain Ethereum with a PoA (Clique) consensus and smart contracts in Solidity.\nDatabase MongoDB to store transactions and access them more quickly. Reconstructible from data stored on the blockchain.\n","permalink":"https://alainnicolas.fr/en/projects/vct-vcoin/","tags":["Kanban","Spring Boot","Java","Ethereum","Solidity","MongoDB","Android","iOS","Angular","Docker","Blockchain","GitLab","IntelliJ IDEA","JavaScript","Jira","NPM","Truffle"],"title":"VINCI Construction Terrassement - VCoin"},{"categories":null,"contents":" \u0026#34;How rare is my NFT?\u0026#34; I recently spent some time with the Bunny Universe community, where this question came up a lot. Each Bunny is unique, but their traits (fur color, accessories, background, etc.) make some rarer than others.\nℹ️ I’m not affiliated with Bunny Universe and I’m not a founder of this ecosystem. However, I’m part of the community and I truly appreciate their work. Of course, this is not financial advice. The challenge: measuring rarity Every NFT has a unique ID, but what really makes one stand out from the rest? Traits.\nSome traits are common, others are rare. Computing rarity means understanding how often each trait appears across a collection and assigning a score accordingly.\nThis led me to explore different tools and methods to compute NFT rarity.\nResearching rarity scoring I explored different platforms and wallets that provide rarity rankings:\nMetaMask \u0026amp; OKX display rarity scores directly in their wallet interfaces.\nThis guide helped me understand different approaches.\nI landed on OpenRarity, which is also used by OKX.\nComputing rarity with OpenRarity OpenRarity provides a Python library that:\nFetches metadata from a collection.\nAnalyzes traits to compute rarity scores.\nYou can check their full documentation here: OpenRarity docs.\nRunning the script I’m not a Python developer, so I used Cursor to set up my environment and tweak the example script.\nHere’s how the process worked:\nFetch metadata: retrieve data for 2,500 NFTs from IPFS.\nCompute rarity scores: use OpenRarity’s algorithm.\nGenerate a JSON file: store the results for easy use.\nThe whole process took about 10 minutes, mostly due to fetching metadata from IPFS.\nWant to try it yourself? I shared my script here.\nFeel free to test it with any NFT collection and let me know what you find!\nI’d love to hear how it works for you! 🚀\n","permalink":"https://alainnicolas.fr/en/blog/nft-rarity/","tags":["Perso","Linea","NFT","rarity","OpenRarity","Ethereum","Python"],"title":"NFT rarity: a deep dive"},{"categories":null,"contents":" Do you own eFrogs on Linea? Do you want to prove you’re part of this amazing community?\nNow you can, thanks to Verax on efrogs.alainnicolas.fr!\nProject origins In April 2024, I built this project in collaboration with my friend Orion. Together, we won second place in the hackathon organized by Linea and eFrogs 🐸.\nAt the time, it was just a simple prototype. But after a few months, it was time to finalise and launch a mainnet version.\nThe result is here: efrogs.alainnicolas.fr, which allows you to verify your ownership of an eFrog NFT using Verax.\nWhat is Verax? Verax is an on chain attestation registry. It enables dApps to store public data, called \u0026#34;attestations,\u0026#34; that are accessible and composable by anyone.\nLearn more on the official website, the documentation, and the explorer.\nThe eFrogs collection on Linea eFrogs (Ethereum Frogs) is a flagship PFP collection on Linea. It includes 2015 frogs, but more importantly, it’s a vibrant community with its own culture and derivative projects, such as the $CROAK token, the eFroglets collection, and the Pond.fun launchpad.\nℹ️ I’m not affiliated with eFrogs and I’m not part of the team behind this ecosystem. However, I’m part of the community and I genuinely appreciate their work. Of course, this is not financial advice. Attest your eFrogs NFTs With efrogs.alainnicolas.fr, you can now attest on Linea, via Verax, to your ownership of eFrogs NFTs. This attestation boosts your credibility within the community and might one day grant access to services requiring proof of ownership.\nSo, does this project serve a specific purpose? Not really. But do you always need a reason to build something? Creating, experimenting with Verax, and contributing to the eFrogs ecosystem is simply fun.\nAnd this project does pave the way for new possibilities. I’m thinking of soul-bound tokens (non-transferable tokens). If your wallet gets compromised, someone else might benefit from your reputation, which isn’t transferable. However, if you sign an attestation proving you owned these NFTs in wallet #1 and prove you control wallet #2, you could reclaim your reputation on wallet #2, thanks to Verax.\nHow to proceed? Go to efrogs.alainnicolas.fr.\nConnect your wallet containing your eFrogs NFTs.\nClick \u0026#34;Issue Attestation\u0026#34; to generate and record your attestation via Verax.\nIn just a few clicks, your ownership of eFrogs NFTs will be attested on Linea, providing immutable proof of your membership in this dynamic community.\nDon’t wait—solidify your presence in the eFrogs ecosystem on Linea!\nLinks to the resources mentioned eFrogs website\neFrogs collection\neFroglets collection\nPond.fun\nLinea website\nLinea’s X account\nVerax website\nVerax documentation\nVerax’s X account\n","permalink":"https://alainnicolas.fr/en/blog/attest-nfts-efrogs-linea-verax/","tags":["Perso","Verax","eFrogs","Linea","NFT","attestations","Ethereum"],"title":"Attest your eFrogs on Linea with Verax"},{"categories":null,"contents":" Are you coding for work and personal projects on the same computer? Then you probably find yourself regularly committing with your personal email address on your professional projects, and vice versa.\nAnd it’s annoying, isn’t it? Constantly having to rewrite the history on one side, the other side, with commits that are already finalized on a protected main branch, etc.\nIt’s time to put an end to this! I propose that you better configure your Git environments. For that, there are two possible approaches.\n1. Repository by repository This is, of course, the technique that provides the finest granularity. You can override the global Git settings project by project using the following commands to be executed at the root of each project:\ngit config user.name \u0026#34;Pro Name\u0026#34; git config user.email \u0026#34;pro@corporate.com\u0026#34; 2. Directory by directory This time, the goal is to separate your projects more drastically to save precious time:\nIn your global configuration (probably ~/.gitconfig), enter the \u0026#39;name\u0026#39; and \u0026#39;email\u0026#39; that you are most likely to use as a priority, let’s say your professional information.\nAdd the following 2 lines after this information to conditionally override them (here, only if you are in the ~/Personal directory):\n[includeIf \u0026#34;gitdir:~/Personal/\u0026#34;] path = ~/Personal/.gitconfig In ~/Personal, add a .gitconfig file that will contain your personal information this time.\nWhen you work in the ~/Personal folder, your commits will automatically have your personal information. No more headaches!\nWhat about projects that already have mixed commits? And if, despite everything, you have repositories that mix commits with your professional and personal information, you can always use the technique I described in this article to clean up!\n","permalink":"https://alainnicolas.fr/en/blog/manage-your-double-life-on-git/","tags":["Git","GitHub"],"title":"Manage your double life on Git!"},{"categories":null,"contents":" Do you also have dozens of JavaScript/TypeScript projects buried in more or less well-organized folders on your computer? You know, those personal projects abandoned after the first commit and long-finished professional projects…​\nAnd at the same time, you see your hard drive filling up? Well, npm has the bad habit of downloading half the internet with each installation…​\nHere’s the solution to free up space without hassle and keep your revolutionary application project that barely started. Four years ago.\nThe state of your hard drive after 4 years of `npm install` npkill is an open-source tool that allows you to quickly and easily delete the node_modules directories cluttering your computer.\nTo do this, it’s very simple, just run npx npkill anywhere on your computer, and it will search for all the node_modules directories.\nIt presents you with the list, including the size of each of these directories, and you can choose to delete or keep them. The end result is often very satisfying, with potentially several gigabytes of freed space!\nFind all the options for this tool in its documentation!\n","permalink":"https://alainnicolas.fr/en/blog/free-up-space-with-npkill/","tags":["NPM"],"title":"Free up space with npkill"},{"categories":null,"contents":" The Context Dependency management is a crucial aspect of any project. In the Node.js world, npm is the standard tool for installing and managing dependencies, but let’s not forget its counterparts yarn and pnpm. The package.json file lists all the dependencies and their versions.\nWhen working on a team project, it’s important to maintain the package.json file consistently. And that’s where the sort-package-json package comes in.\nsort-package-json It allows you to sort the package.json file alphabetically based on package names. This ensures consistency in the order of dependencies, making the file more readable and maintainable.\nIn addition to dependencies, which are sorted by default if you’ve only installed them via command line (npm install XXX), sort-package-json also sorts other fields in the file, such as author, project name, version, etc.\nFor this purpose, the package utilizes predefined rules defined here.\nUsage Using it is simple! Just run npx sort-package-json and you’re done!\nBut you can also integrate it into a broader standardization strategy for your project, for example, by adding it to the tasks executed by your Git hooks using Husky, as described in the documentation here.\n","permalink":"https://alainnicolas.fr/en/blog/sort-your-dependencies/","tags":["Nodejs","NPM"],"title":"Sort your dependencies!"},{"categories":null,"contents":" Now that your blog is online and you’re motivated to write articles, it’s time to take it to the next level. Regardless of what anyone says, having readers and receiving feedback (preferably positive) boosts your ego and encourages you to do more and improve. But don’t forget, your blog is primarily YOUR blog.\nConsider a Post as a Reminder I believe it’s an excellent way to take ownership of your blog while increasing the publishing frequency: every time you learn something, you should write a post about it. Your blog then becomes an extension of your brain, capturing everything you can never seem to remember.\nFor example, if you frequently search for \u0026#34;How to center a div in CSS,\u0026#34; it’s probably time to note it down on your site, and the answer will always be readily available. And who knows, you might even remember the solution along the way!\nThe same goes for commands you constantly forget. What’s the order of parameters for the cp command again? In short, you get my point: you can transform your blog into a kind of reminder list. Just be careful not to completely abandon more in-depth articles that appeal to a different audience.\nIn any case, it’s an excellent way to combat a lack of inspiration, maintain the pace you’ve set for yourself, and keep your SEO up to date by regularly updating your site!\nLink Article(s) and GitHub Repository Are you writing a technical article and including snippets of code in the middle? Why not take it a step further and create a demo project?\nThe goal is twofold: to showcase real, functioning examples and to build a public portfolio to showcase your skills. Personally, I consider the development of this project as part of my (lengthy) thinking process, so I tend to code before writing the article. But of course, during the writing process, there always comes a point where you realize the code needs improvement, so you go back to it.\nBy starting with your use case or technical demonstration, it becomes easier to illustrate your article in a very concrete manner. In fact, I believe it’s the easiest way to write a highly technical article. In doing so, you ensure that the code shown is truly functional, which is not always the case when blindly copying code between paragraphs of text.\nCross-Posting It’s interesting to expand your reach by posting your article on multiple platforms. You can publish it on your personal blog, your company’s blog, Medium, LinkedIn, and more.\nIn any case, don’t forget to specify a canonical URL to avoid any issues with duplicate content. This way, you prevent potential SEO penalties from search engines that may consider your content as mere duplication of external content.\nAnd, of course, share your article on social media to reach an even wider audience and drive traffic to your page!\nThis article concludes a series on maintaining a blog, from setting up\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-take-it-to-the-next-level/","tags":["Blogging"],"title":"Blog It Yourself - Take it to the next level"},{"categories":null,"contents":" As we discussed in the previous article, there are many obstacles that prevent us from writing articles. Here, I share my experience on how to overcome them.\nLet’s explore how to overcome the dreaded writer’s block for developers! And let’s tackle the impostor syndrome that often immobilizes us.\nTo be completely transparent, I frequently face these obstacles myself. First and foremost within myself, but also among the people I’ve had the pleasure of working with for years. Here are some tips that I try to follow and share. Keep in mind that what works for me may not work for everyone…​\nThink a lot, write quickly This is probably the best description of my approach. It often takes me days, or even months, to write an article. A topic needs time to truly mature, and I often work on it in the background. You know, like those bugs you solve at random times, in the shower, and so on.\nBut since we work all day, it’s often difficult to have a continuous train of thought that we can pick up where we left off. So, we need to take notes, bookmark articles to easily find them later—anything that helps us gather information when we find a spare moment.\nThen, it’s perfectly fine to create an outline for your article, sprinkling in some links as references and anchors for your future self.\nIt’s a holistic process that takes more or less time depending on the individual and the subject matter, but I find it particularly suitable for our already busy lives. And then…​ it’s time to dive in! And at that point, the writing can flow very quickly!\nIn contrast to the period of reflection when we gather sources and ideas, the writing comes naturally because everything is already in our minds! For those who enjoy writing, it can even be an exhilarating moment: in a matter of minutes, you can write several dozen lines without even noticing the time passing because everything is already in place. It’s a kind of liberation, as everything finally falls into place and our brains can empty at the same time.\nThe first drafts of articles that have been prepared well in advance are often the final versions with only minor adjustments. It’s not uncommon for me to have only a few days between starting the writing process and publishing a post, allowing time to review it with a fresh perspective. And having someone else proofread it to avoid sharing too many mistakes.\nSetting goals We know that procrastination is even stronger when we don’t feel like doing a task. And yet, in our case, it’s important to overcome our usual shortcomings. I recommend setting goals for yourself, if possible, achievable ones. For example, why not aim for one article per month to start?\nAt the same time, during slow periods, it’s very interesting to create \u0026#34;reserves\u0026#34; of posts to publish later. That way, you can procrastinate a bit on the upcoming ones!\nTo maintain the pace, you can alternate between several types of articles, some taking longer to write than others. This allows you to catch your breath between two particularly challenging tutorials, for example.\nDon’t hesitate to split into multiple articles Often, when tackling a subject to write an article, we discover new aspects or elements that we didn’t master. That’s cool, it means we’re learning new things! But in return, we end up with a mountain of topics to address, which extends the duration of our thinking process and the writing time, but above all results in a lengthy article that won’t be easily digestible.\nIn that case, don’t hesitate to split your article into several parts. If possible, in a logical manner (based on the outline mentioned earlier, for example). Moreover, it’s also an excellent way to increase the number of publications, and consequently improve your referencing, personal satisfaction, etc.\nHowever, don’t try to stretch it out just for the sake of it; readers will notice. It’s better to aim for conciseness and get straight to the point…​ something I struggle with a lot!\nDon’t hesitate to tackle subjects you don’t master 100% As we’ve seen, imposter syndrome often tends to inhibit us. But we have to take the plunge…​ Even with all the courage in the world, it’s difficult to feel sufficiently legitimate to write an article on a subject we don’t feel we fully master.\nHowever, if only experts wrote articles, there would be very few publications to guide us when we seek help on the internet! So, should we just dive in and write anything? No! It’s entirely possible to highlight the points we don’t understand, revisit them later, etc. Firstly, because we never know everything, and secondly, because it’s an excellent way to learn!\nBy doing research, you’ll acquire knowledge you didn’t have before, simply because you didn’t need it during your development phase, for example. But writing your article will help you improve your skills, it’s a double win!\nLater on, by continuing to work on the same subject, you might understand new things, discover that you weren’t following best practices…​ and that will allow you to write an additional article, showcasing the evolution of your knowledge. Another opportunity to do so is during a major update of a tool you were presenting, for example!\nWhat do you think of these tips? Are you going to take the plunge?\nI propose going a step further to improve the quality of your blog posts in an upcoming article!\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-my-experience/","tags":["Blogging"],"title":"Blog It Yourself - My experience"},{"categories":null,"contents":" You have a beautiful blog, perhaps thanks to the previous article, but you struggle to write articles for it? It’s normal…​ There are numerous and real obstacles.\nThere are many \u0026#34;excuses\u0026#34; for not getting started, and most of the time, we believe in them wholeheartedly! Let me share the three main reasons I’ve heard for not writing articles.\nI don’t have the expertise to write articles Clearly the main reason that prevents all of us from getting started. The infamous \u0026#34;Imposter Syndrome\u0026#34;…​\nI used to believe it was reserved for the more junior developers among us, but the truth is…​ not at all! How do I know? Firstly, because I still feel it, and secondly, because I regularly hear experienced developers talk about it.\nSo the issue lies not in actual experience but in the perception of it! And that’s important to note! It simply means that there is no perfect moment in one’s career to start writing an article, and that you, too, can do it.\nFurthermore, it’s essential to realize that the perspectives of a junior developer and a senior developer are different enough that both can express themselves without invalidating each other. If you still had doubts, there is room for everyone on the internet…​\nI don’t have any article ideas to write about Ah, we often hear this one too! And yet, it’s probably the least valid reason…​\nAll developers know how to do things. Many things, in fact, with various tools, some more exotic than others. There are no two identical projects; there are always different contexts, clients with their own characteristics, and so on. Therefore, there is an endless source of ideas to fuel us!\nFirst and foremost, think about sharing your experiences. Firstly, because we all have \u0026#39;experiences\u0026#39; that others don’t. Secondly, because you can’t be wrong when talking about what you’ve seen and lived. No one is better positioned than you to talk about your personal experience!\nAnd then, we can imagine many small topics that deserve our time! For example, why not write a short tutorial on a tool you regularly use? Maybe there are already many tutorials out there? Perhaps, but no one uses it the same way you do, with the shortcut that saves you time, etc.\nIn fact, when you think about it, we ALL have things to share. Just think about everything you’ve done in the week, removing the barriers you easily put up!\nI’m afraid of saying stupid things In line with the previous two reasons, obviously. Adding a dose of shyness…​\nSo, the simplest solution is to have someone proofread your work. Someone more senior if it reassures you, but anyone will do, really! A junior can specifically tell you if the article is approachable, if there’s a missing definition to understand everything, etc.\nIn return, you must be willing to accept advice, remarks, syntactical corrections…​ Yes, our ego sometimes catches up with us, even in the midst of imposter syndrome!\nBut even if you don’t feel confident enough to start, fearing to lose face, what do you really have to lose? Not much, unless you heavily lied on your CV! If I realize that I’ve only said stupid things in my previous article, that’s perfect—I already have the idea for the next one that will have to explain how wrong I was!\nAnd for more basic mistakes, a correction can be quickly published, so don’t panic. Unlike Twitter, you can take it back, and the correction will probably go unnoticed!\nThere are many obstacles, and they vary for each of us, but they also depend on our environment and its ability to help us grow. In the next article, I share my experience on the subject!\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-what-holds-us-back/","tags":["Blogging"],"title":"Blog It Yourself - What holds us back"},{"categories":null,"contents":" I was discussing the choice and use of Ghost as a blogging tool. After 18 months of use, for the reasons mentioned at the end of the [previous article](link:/en/blog/blog-it-yourself-ghost), I decided to change platforms. And also because after the free 12 months of the AWS instance, it was necessary to pay, even if it remains small amounts.\nStatic Site Generator? First and foremost, it is important to understand that this is not just a technical solution change, but a change in mindset. WordPress or Ghost are software that expose a website along with writing and administration tools. In contrast, Hugo is just a site generator. It takes a theme and a list of content and generates HTML pages …​ that’s it!\nIn other words, all the creation, management, and writing of the site is done on the owner’s computer. In return, the generated pages do not require an API or any server to feed them. So, the site is not \u0026#34;hackable\u0026#34; as such. It all depends on how these HTML pages are hosted, of course.\nWhereas WordPress and Ghost provide real-time publishing of the modifications that are made and thus lose the previous version, Hugo simplifies things greatly: it is very easy to version the \u0026#34;text\u0026#34; files that make it up, via Git.\nTherefore, you can version the site’s content (blog posts, etc.), as well as the site’s theme itself. This allows you to revert back after an unfortunate update, for example, without having to remember the 36 modified options in a WordPress installation.\nIt should be noted that there are dozens of static site generators with different characteristics, written in different languages, etc. I invite you to check the list on the [JAMStack website](https://jamstack.org/generators/) (and also learn about JAMStack, although it is not precisely the topic here).\nAmong its direct competitors (Jekyll and Gatsby), Hugo stands out by a small margin in terms of GitHub stars, if that is an important indicator when making your choice…​\nThemes Let’s say you decide to go with Hugo. Once the tool is installed on your computer, you still need to find a theme that fits your needs and tastes. Fortunately, everything is provided on the [official themes page](https://themes.gohugo.io/). With good theme categorization and demonstration sites, it is quite easy to find what you’re looking for.\nThat’s how I found the theme created by [Eddie Webbinaro](https://webbinaro.dev/), aptly named [Resume](https://themes.gohugo.io/themes/hugo-resume/).\nCan’t find what you need? Don’t panic! While you can create your own theme (as described in [this post](https://retrolog.io/blog/creating-a-hugo-theme-from-scratch/), for example), know that many themes are not listed on the official page, so you can do your own research.\nFor example, I found a Hugo version of Ghost’s default theme with a quick search. That’s how RoxTheCasbah didn’t even change its appearance when it migrated from Ghost to Hugo! And if you’re interested, this theme (Casper 3) is available here.\nExcerpt from the Hugo themes list Similarly to creating your own theme, it is also possible to modify an existing theme. However, be mindful of the license attached to it!\nFor example, I didn’t need certain sections offered by the Resume theme I mentioned earlier, so I removed them and added other pages, etc. The same goes for improving the display of a blog post according to my preferences.\nHowever, it should be recognized that modifying the theme can quickly deviate from its original version and make it impossible to update locally if the author modifies it on their end.\nAdditionally, Hugo comes with very handy default behaviors, but they can also be quite restrictive. For instance, by default, Hugo uses the description front matter entry in a Markdown document to generate the \u0026lt;meta name=\u0026#34;description\u0026#34;\u0026gt; tag, which represents your page on search engines. Be careful if you named it something like extract (which can be very logical) because the page will simply have no description, which is highly detrimental to SEO on Google!\nContent Is your theme in place? Now it’s time to add content to your site!\nAnd for me, this is the best moment to realize the power offered by Hugo: everything can be \u0026#34;markdownified.\u0026#34; In other words, all the content of your site can be written in text files in the default Markdown format. This means that to modify the content of your \u0026#34;Home\u0026#34; page, there’s no need to search for which HTML file contains the information. Simply find the home.md file, which is likely located at the root of your project.\nI mentioned this in an article back in early October 2021, where I discussed how it’s possible to go beyond Markdown and use the AsciiDoc language. It’s also possible to have part of the content in Markdown and the rest in AsciiDoc, which is convenient when starting with an existing site.\nAsciidoctor Logo The Hugo documentation is very well done, and the large community maintains a help forum with valuable resources. For example, you can find the necessary information on implementing a tagging system to categorize your articles or the code required to embed YouTube videos on a page.\nThe site generation is incredibly fast, taking only a few seconds for a site with several hundred pages. This is a selling point emphasized by Hugo and is very practical when writing an article or modifying the structure of a page since changes are displayed instantly.\nHosting \u0026amp; Deployment As mentioned, Hugo only generates HTML pages, so you need to take care of hosting them. If you have a server at home, this can be done without any issues, especially since you don’t need a high-powered machine to serve such resources. For others, there are free solutions!\nLet’s first talk about GitHub Pages (and its counterpart on GitLab, GitLab Pages). These services are provided for free by your favorite source code management platforms.\nThere are differences between the two; the configuration is not exactly the same. However, the end goal remains the same: having a web page (or a complete site) hosted on a remote server accessible from anywhere.\nAt the same time, you benefit from the performance and reliability of these major companies. If our page becomes inaccessible, there’s a high chance that many other sites are also inaccessible at the same time.\nAs an example, my site and RoxTheCasbah are hosted on GitHub Pages.\nWe can also mention Netlify, which offers a hosting service dedicated to static sites. They provide a free version that is sufficient for hosting a site, and a paid version for more advanced features (team collaboration, etc.). By integrating with your GitHub repository, for example, Netlify can detect changes, build the new version of the site, and deploy it immediately.\nNote that automatic deployment is not always a good idea, as you may end up with an unfinished article published for everyone to see. Therefore, consider manual deployment for your production site while keeping automatic delivery for a staging version, for example.\nI use Netlify to deploy different versions of my site, such as versions with new features to test under real-like conditions or articles still in draft. However, I can also easily switch the \u0026#34;production\u0026#34; version from GitHub to Netlify with a simple DNS redirection change, in case the former service becomes paid or experiences a major outage.\nOnce deployed on GitHub Pages, the site is available at the username.github.io address, such as alainncls.github.io in my case. It’s not yet very personalized…​ And let’s not forget about the default Netlify addresses that look like stoic-franklin-0f67b8.netlify.app or reverent-stonebraker-e22885.netlify.app!\nTherefore, it’s necessary to \u0026#34;mask\u0026#34; these addresses with a cleaner domain name, which is a paid operation. By the way, be cautious of offers that seem too good to be true—don’t purchase your domain name from just anywhere! Personally, I use Google’s service, Google Domains.\nConclusion To summarize the positive aspects of Hugo, we can mention:\nImproved security,\nVersioning of the theme and content,\nFree usage,\nFast site generation, even for sites with hundreds of pages,\nEasy free hosting of HTML pages,\nAnd finally, the thriving community around the tool.\nHowever, it’s important to consider some downsides:\nThe difficulty, or even impossibility, of changing the theme if the original one has been heavily modified,\nThe requirement to adhere to restrictive standards; otherwise, extensive manual development work is needed,\nAnd the increased risk of making SEO mistakes for the entire site.\nNow that we have explored various technical solutions for creating a blog, the next article will cover how to write posts, even when you feel like you have nothing to say, in the next article.\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-hugo/","tags":["Blogging","Hugo"],"title":"Blog It Yourself - Hugo"},{"categories":null,"contents":" In cleaning up my repositories on GitHub, I came across old projects, some of which are ancient. Let’s not dwell on the shame one can feel when looking at code written almost 10 years ago; it’s inevitably very ugly…​\nBut one thing caught my attention: many commits are not associated with my username or do not display my profile picture. Strange, since the most recent ones seem to correspond to my username and show my picture.\nWhat changed in the meantime?\nThe answer came quite quickly: I have often changed my email address over the years (school, first job, new personal address, and so on). Now, I know that you can modify the author of a commit, their username, or email address. But with hundreds of commits across dozens of repositories, well…​ life is too short, you know?\nIn fact, it’s all about having the right tools.\nBy hand ... it\u0026#39;s long With the right tools ... it\u0026#39;s better I found the right tool on Stack Overflow^ (note that this answer appears on dozens of posts). It all lies in the filter-branch command, which is capable of traversing all the commits in the project and applying modifications to them.\nCan you sense the potential? Yes, it could destroy your history, so let’s proceed with caution, shall we?\nIn my case, I needed to modify the commits written with a certain email address and replace it with my current email address and username to bring everything into harmony. The command line looks like this:\ngit filter-branch -f --env-filter \u0026#39; OLD_EMAIL=\u0026#34;old.email@example.com\u0026#34; CORRECT_NAME=\u0026#34;New Name\u0026#34; CORRECT_EMAIL=\u0026#34;new.email@example.com\u0026#34; if [ \u0026#34;$GIT_COMMITTER_EMAIL\u0026#34; = \u0026#34;$OLD_EMAIL\u0026#34; ] then export GIT_COMMITTER_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_COMMITTER_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi if [ \u0026#34;$GIT_AUTHOR_EMAIL\u0026#34; = \u0026#34;$OLD_EMAIL\u0026#34; ] then export GIT_AUTHOR_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_AUTHOR_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi \u0026#39; --tag-name-filter cat -- --branches --tags Once these modifications are made locally, you still need to push them to the remote repository:\ngit push -f --tags origin HEAD:main And clean up the local references:\ngit update-ref -d refs/original/refs/heads/main You will then see that the history correctly displays the commits as ours:\nExcerpt from an old Java 7 project And here’s a little boost to the ego: suddenly, there are more commits counted by GitHub on your profile page! 😉\n","permalink":"https://alainnicolas.fr/en/blog/rewrite-your-git-history-and-fast/","tags":["Git","GitHub"],"title":"Rewrite your Git history (and fast)"},{"categories":null,"contents":" I mentioned it in my article about choosing a blog platform, the first choice for creating the blog RoxTheCasbah was Ghost.\nSo here’s an experience report on the installation, configuration, and usage of Ghost, which presents itself as an alternative to WordPress, simpler, lighter, and more modern.\nHosting Let’s leave aside the paid version of Ghost that offers hosting by the company, and thus minimal configuration for the user. Let’s focus on the free and open-source version of the tool, to be self-hosted.\nBecause there’s never a bad occasion to learn something, we chose to host the blog on a free AWS instance to avoid any unnecessary cost while waiting to know the site’s traffic.\nAWS Offer Indeed, AWS offers a free version for one year of one of the smallest machines on its EC2 service. The specifications of this machine generally meet the prerequisites recommended by Ghost in its documentation, so we didn’t go any further!\nInstallation Again, the goal is to keep it simple and fast, thanks to the fairly comprehensive documentation provided by Ghost, describing the manual installation on different OSes and configurations. We followed the Ubuntu guide.\nNote that Ghost provides a small utility to manage your site, from installation to updates: Ghost CLI.\nWith this tool installed (npm i -g ghost-cli), all that’s left is to run the ghost install command, answer a few questions (site name, URL, etc.), and you’re good to go:\n$ ghost install ✔ Checking system Node.js version ✔ Checking logged in user ✔ Checking current folder permissions ✔ Checking memory availability ✔ Checking free space ✔ Checking for latest Ghost version ✔ Setting up install directory ✔ Downloading and installing Ghost v4.2.2 ✔ Finishing install process ? Enter your blog URL: https://myblog.com ? Enter your MySQL hostname: localhost ? Enter your MySQL username: mycoolname ? Enter your MySQL password: [hidden] ? Enter your Ghost database name: ghost_prod Launching Once the site is launched (ghost start), a quick navigation assures us that everything seems to be working as expected. It is now possible to create an administrator account and one or more \u0026#34;writer\u0026#34; accounts from the site’s interface. Article writing can start smoothly!\nLet’s be clear: it’s great! With minimal effort, we have an operational site, a comprehensive back office, and a friendly text editor.\nI emphasize the text editor; it is on par with what you can find on Medium, and far superior to what WordPress offers. And you can even write your articles in Markdown!\nI was just talking about Markdown in a series of articles about \u0026#34;Documentation as Code\u0026#34;.\nImprovements If the website obtained at the first launch is fully functional, it should be noted that many optimizations are almost mandatory to achieve a fast and well-referenced blog.\nNGINX, Performance, and SEO Having a blog is good, having a blog that is indexed by search engines is even better!\nBy following the installation process, Ghost uses Nginx as a reverse proxy. By modifying its default configuration, we can achieve higher performance than the already decent performance at the initial launch. These performance optimizations include image optimization and browser caching.\nRather than paraphrasing already comprehensive articles, I encourage you to take a look at [this](https://bironthemes.com/blog/configure-nginx-for-ghost/) and [that](https://web.archive.org/web/20230606152944/https://dzone.com/articles/optimize-ghost-blog-performance-including-rewritin) for more details.\nComments and Contact Once the blog is online and the first articles are written, it is natural to have new needs.\nFor example, how to get a comments module under the articles to engage with the growing community? One solution that came to our mind quite easily is Disqus, although there are other tools available. The integration of this tool is almost native in Ghost’s [documentation](https://ghost.org/integrations/disqus/), so the setup is quick.\nAnother common feature is a contact page to allow visitors to contact the blog author in a more private way. Again, several third-party services can be used for this purpose. We chose Formspree, which allows managing different types of forms. Once again, the Ghost [documentation](https://ghost.org/integrations/formspree/) has a dedicated page for it.\nIt’s worth noting that Ghost’s [documentation](https://ghost.org/integrations/) covers numerous integrations, and most of the needs seem to be covered!\nThe Many Features Ghost provides a list of features on a dedicated [page](https://ghost.org/feature-index/), and it’s safe to say that they are quite extensive.\nAt first, it’s very exciting: the blog I’m setting up will be able to do a lot of things on its own, which is great. But then, we can start to think that it’s not as lightweight and simple as initially promised…​\nIt\u0026#39;s a complex system, are you following me? Among the features that outshine WordPress, I would mention the management of a publication workflow, including the concept of reviews and multiple authors. This is very interesting, especially for a semi-professional blog where publications need to be well-polished from the moment they are published.\nAnd among the features that might be considered excessive, there’s the management of free and paid members. This is only relevant for a fraction of online authors; not everyone needs a system worthy of a national newspaper.\nHonestly, would you pay to read me? (If the answer is \u0026#34;yes,\u0026#34; send me a check, no problem!) Well, maybe setting up a newsletter to be notified of new publications is a reasonable idea.\nConclusion Ghost is a good solution to quickly set up a blog and focus on writing articles. I was convinced because with a few clicks and command lines, I obtained a very comprehensive blog.\nHowever, in its self-hosted version, Ghost is not without its challenges:\nEnsuring the security of the AWS instance, Nginx, and Ghost itself\nThe default configuration is not sufficiently optimized\nThere are frequent updates (55 in 2021, including a major version, fortunately [well-documented](https://ghost.org/docs/changes/))\nIt is a bit heavy for the free AWS EC2 instance\nLet’s also mention its (too) many features that go beyond its original slogan: \u0026#34;Just a blogging platform\u0026#34;.\nIn the [next article](/blog/blog-it-yourself-hugo), I will tell you how I switched from Ghost to Hugo, with a radical change in philosophy.\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-ghost/","tags":["Blogging"],"title":"Blog It Yourself - Ghost"},{"categories":null,"contents":" In my opinion, all developers should have a blog! But how do you get started? Let’s explore the tools and techniques of writing.\n","permalink":"https://alainnicolas.fr/en/series/blog-it-yourself/","tags":null,"title":"Blog It Yourself"},{"categories":null,"contents":" When my partner came up with the idea of creating a blog to share her passions, the question of technical choice quickly arose. Among developers (or \u0026#34;geeks,\u0026#34; as some would say), we couldn’t avoid the debate…​\nYes, for years and for a good while longer, anyone looking to create a website quickly is likely to turn to WordPress. Simply because the platform has proven itself over a long period and is by far the most widely used. More than 40% of currently active websites are based on WordPress (source : Kinsta). That’s simply enormous…​\nThe jungle However, does everyone really need such a complex powerhouse like WordPress? And does everyone want to have the same interface or the same security vulnerabilities? Not so sure…​\nWe wanted to avoid the bloated factory with standardized output, and that’s how our journey into the wonderful world of CMS (Content Management System) began.\nThere are those that are e-commerce-oriented like Magento, those that are outdated like Typo3 or Drupal, those that are (too) comprehensive like Joomla…​\nCMS designed for blogs Digging deeper and filtering further to find solutions that are truly dedicated to blog creation, we discover other, perhaps less known, options.\nSquarespace Demo Site There’s the giant Medium, which allows you to write articles and customize their appearance, but doesn’t let you host them wherever you want. Or Squarespace, which offers visually appealing sites but is actually more suited for graphic content creators.\nCost And before launching a site, it’s difficult to know if it will be successful and therefore worth spending money to create, host, and maintain it. One of our significant criteria was, therefore, to find a free solution, at least initially, to start our journey.\nSerendipity Official Blog Special mention goes to the deliciously retro look of Serendipity, the intentionally \u0026#34;not mainstream since 2002\u0026#34; blog. But, as long as we’re at it, let’s stay anchored in the 21st century…​\nSo, a free solution, yes, but not at the expense of the final output.\nSimplicity To be thorough, we didn’t intend to limit ourselves to these criteria. There was one more important criterion: simplicity. Writing articles about your passions should remain a pleasure and an easy experience. Therefore, we needed a solution that offers the simplest possible text editor.\nAnd what’s better than using our favorite daily tools? As developers, we’re used to writing documentation in Markdown or Asciidoc, so why change our habits?\nThere are solutions available to generate standard web pages from these languages, such as Jekyll, which is said to be the most popular, or Hugo, which is said to be the fastest. The process is simple: you define a presentation template, write your article in a text file, and the software generates web pages that can be deployed on a server.\nThis approach provides speed (barely a few seconds to generate a site with dozens of articles using Hugo) and security (basic HTML files cannot be hacked).\nHowever, even though the communities are vibrant, there are still few truly modern and appealing templates available. Once again, it would be a shame to sacrifice the final output of the site in the name of simplicity.\nHosting Writing articles, whether it’s about knitting or blockchain, means sharing a part of yourself and your ideas. So, it’s better to have control over these articles…​\nTherefore, after the criteria of cost and simplicity, the criterion of hosting control became essential for us. While many platforms offer to host your WordPress site and provide a domain name, email addresses, and various more or less useful services, they won’t truly give you control over your own data, your own ideas, your own content…​ And what protects you from a price increase? New conditions regarding access to your old articles?\nMastering the hosting of your blog also means taking responsibility for security (configuration, access keys, etc.), which is not entirely trivial and sometimes requires technical knowledge.\nAWS Logo\nMajor hosting providers like AWS or OVH offer free plans that often suffice to host a blog.\nAnd for the most motivated and equipped, it is, of course, possible to self-host your blog, to achieve the ideal of controlling your own data!\nOur choice Finally, a choice had to be made…​\nWe turned to Ghost, a CMS (like WordPress) with a simple and efficient text editor that allows you to write articles in Markdown or more traditionally with buttons for bold, italic, etc.\nTo start, we chose its default theme, which seemed to meet our needs quite well. However, note that there are many other themes available.\nGhost comes in two versions:\nPaid (see pricing) to not worry about hosting, updates, and get support. Free, for users who want to self-host their site but need to take care of updates, security, etc. We chose the second option, which has an intermediate level of difficulty.\nRegarding the three main criteria discussed in this article, Ghost seems to tick the boxes:\nCost: it offers a free solution for a slightly technical audience. Simplicity: its original motto was \u0026#34;Just a blogging platform\u0026#34; (in contrast to WordPress, which can do almost everything). Hosting control: it aligns well with our desire to keep control over our articles. So, all that was left was to dive in and publish the first version of the blog!\nIn the next article, I will tell you about our blog setup, from hosting it on AWS to optimization.\n","permalink":"https://alainnicolas.fr/en/blog/blog-it-yourself-the-choice/","tags":["Blogging","WordPress"],"title":"Blog It Yourself - The choice"},{"categories":null,"contents":" Let’s start with a little quote:\n\u0026#34;New year, new me\u0026#34; — Too many people,too often The Good Resolution Let’s avoid resolutions that we won’t keep. Nevertheless, let’s try to improve ourselves regularly…​\nTo start the year 2022, I have decided to do without Google Analytics to track visits to my site. Firstly, because avoiding Google is always a good idea, and secondly, because it is possible to achieve almost the same results without tracking our visitors.\nI mentioned it in an article in October, to comply with GDPR and obtain consent from our visitors to be tracked, we ourselves must consent to implementing a system (such as a popover or banner), which is not only restrictive but also degrades the user experience.\nSmile, you\u0026#39;re being tracked Searching for an Alternative My criteria were:\nA free solution (I don’t have enough traffic to justify any investment other than my time)\nA solution without cookies (there’s no point in leaving Google to provide too much data to another provider)\nA SaaS solution (in line with my website hosted via GitHub Pages)\nThere are dozens and dozens of articles listing alternatives to Google Analytics. More or less accurate, more or less up-to-date, more or less interested in your clicks on affiliate links…​\nI spent some time on this and eventually realized that the best summary can be found in the form of a GitHub repository: Awesome Analytics, which lists over a hundred analytics tools categorized by types.\nMy Solution: Insights.io And my choice is a tool that isn’t even listed there! Note to self: remember to open a PR to propose adding the chosen solution to the list!\nThe motto of Insights.io I finally chose Insights.io, which offers:\nFree usage up to 3000 page views per month (I’m far from that)\nNo cookie placement in the visitor’s browser\nA proper interface on their website\nOf course, the day my site becomes very famous (ah, ah!), I will have to pay or switch to a different tool. And my statistics are not hosted on my own server but on Insights.io’s servers; they don’t offer self-hosting.\nSo, it’s definitely not the perfect solution. Nevertheless, the tool is very simple and works well.\nImplementation To get started, it’s very simple! Once the account is created, we add a \u0026#34;project\u0026#34; to our dashboard. We then get a code (exactly how Google Analytics works), which serves as an identifier.\nThen, we need to add the JavaScript library to all our pages, which will send the information from our site to Insights.io’s servers. There are two methods described in the GitHub repository of the library:\nInstallation via npm install/yarn\nnpm install insights-js Directly from the CDN unpkg:\n\u0026lt;script src=\u0026#34;https://unpkg.com/insights-js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Obtained Statistics As I mentioned, the obtained statistics are much less comprehensive than those provided by Google Analytics. Here, there’s no geolocation of your visitors or incomprehensible conversion funnels without a PhD in marketing; we go straight to the essentials!\nFirstly, the graph of the number of pages visited over 24 hours, by week, month, etc.:\nInsights.io Dashboard Preview - The Graph And because things are well done, we still have details about the visits that have taken place, such as the most viewed pages, time spent on the site, and the OS, browsers, and screens of the visitors. More than enough for a site like mine!\nInsights.io Dashboard Preview - The Details Bonus: Robot Filtering When I implemented Insights.io on my site, I was surprised to see the persistence of visits from a certain \u0026#34;PetalBot\u0026#34;:\nStatistics overwhelmed by a robot After some investigation, it turns out to be a search engine indexing bot for PetalSearch, a search engine offered by Huawei. And it’s safe to say that it indexes! Often.\nIn principle, I have nothing against this indexing, which may bring me some visits someday. But it significantly skews my statistics…​\nTherefore, I took the liberty to fork the insights-js library and add robot filtering based on the \u0026#34;user agents\u0026#34; they display. Of course, one needs to know the bad user agents…​ And once again, a Github repository comes to the rescue, particularly this file!\nMy version of the library with this filtering is available here. However, this time you have to build the library yourself; I won’t do everything, you know! 😘\nAs a result, I was able to remove the import of the Google Analytics script, as well as the Tarteaucitron library I mentioned a few months ago, which can only speed up the loading of my site. And what benefits from that…​ is being indexed by Google!\nYes, because without Google to index me, who would visit my site? 😅\n","permalink":"https://alainnicolas.fr/en/blog/goodbye-google-analytics/","tags":["Insights","Tarteaucitron","Blogging"],"title":"Goodbye Google (Analytics)!"},{"categories":null,"contents":" After the general presentation of Spring REST Docs, I propose to go further by exploring small improvements that will make all the difference!\nWe will continue to base ourselves on my demonstration project available on GitHub: demo-spring-rest-docs.\nExpose the documentation \u0026#34;automagically\u0026#34; As we have seen, we are now able to generate an HTML page containing the documentation of our API. But that’s not the best way to make it available to API consumers, we can do better than that!\nAnd what better place than the Spring Boot application itself? For that, some configuration is required, thanks to the Maven plugin maven-resources-plugin.\n\u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-resources\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.outputDirectory}/static/docs/\u0026lt;/outputDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}/generated-docs\u0026lt;/directory\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; This code copies the contents of the target/generated-docs directory to a target/static-docs/docs directory. In other words, the generated HTML page is placed in a somewhat special directory, since static is automatically served by Spring. If you want to learn more about this feature, Baeldung has a dedicated page: Baeldung.\nWith this configuration, if you build the application (mvn package), you will get a JAR file that you can launch (java -jar demo-spring-rest-docs-0.0.1-SNAPSHOT.jar). The API will be exposed, but the documentation will also be accessible at http://localhost:8080/docs/index.html. So as soon as the application is deployed somewhere, its documentation will be present!\nHide oversize elements by default Sometimes we need to document endpoints that return relatively long responses. This can make the documentation page quite heavy. Fortunately, AsciiDoc has a tag to solve this problem: [%collapsible].\nEverything inside a tagged block is hidden by default, and the reader needs to click on it to expand it:\nLarge Element (click here to expand) [%collapsible] ==== Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus eget mollis neque. Etiam tempor lacinia lorem eget auctor. Quisque accumsan leo a tincidunt hendrerit. Nam eros ante, scelerisque eu tempus et, vestibulum luctus turpis. Donec id nisi risus. Nullam eu purus vulputate velit pharetra hendrerit. Donec varius, velit vitae aliquam interdum, dui sapien faucibus ipsum, et sollicitudin ligula magna quis velit. Donec luctus sed nisi ac blandit. Phasellus sodales mattis pharetra. Duis dignissim tellus nibh, quis imperdiet turpis pharetra et. Phasellus purus odio, pulvinar vel urna vel, consequat vulputate metus. Nunc elementum ornare eleifend. Pellentesque non dapibus ipsum. Nunc malesuada varius elit, auctor tristique nisl pellentesque sed. Vestibulum justo mauris, molestie ut tincidunt a, condimentum ac turpis. Aliquam eu interdum orci. ==== This is how we find, for example, in the documentation of our demo application:\n.Response [%collapsible] ==== include::{snippets}/getAllCompanies/http-response.adoc[] ==== Simplify request/response descriptions To describe the request related to an endpoint, there’s nothing better than the http-request snippet. It contains the URL, the HTTP verb, headers, and optionally the body.\nHowever, headers are often numerous, especially those related to security. The same applies to the response, with many headers added by the framework and its tooling.\nIf these headers have business meaning, that’s great. But if they only create \u0026#34;noise,\u0026#34; it’s better to remove them from the description (both in the request and the response) to make it clearer.\nI propose a small utility (ControllerTestUtils) to avoid repeating the same header removal in each test:\npublic class ControllerTestUtils { static OperationRequestPreprocessor preprocessRequest() { return Preprocessors.preprocessRequest(removeHeaders(\u0026#34;Content-Length\u0026#34;, \u0026#34;X-CSRF-TOKEN\u0026#34;), prettyPrint()); } static OperationResponsePreprocessor preprocessResponse() { return Preprocessors.preprocessResponse(removeHeaders(\u0026#34;Content-Length\u0026#34;, \u0026#34;Pragma\u0026#34;, \u0026#34;X-XSS-Protection\u0026#34;, \u0026#34;Expires\u0026#34;, \u0026#34;X-Frame-Options\u0026#34;, \u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;Cache-Control\u0026#34;), prettyPrint()); } } Feel free to remove specific headers according to your preferences or needs.\nFor example, let’s consider the Content-Type header in the request to delete a Company. You just need to add it to the list of headers to remove (see the code above).\nBefore: DELETE /companies/ID_1 HTTP/1.1 Content-Type: application/json;charset=UTF-8 Host: localhost:8080 { \u0026#34;id\u0026#34; : \u0026#34;ID_1\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34; : \u0026#34;2021-11-06T11:03:53.066+00:00\u0026#34; } After: DELETE /companies/ID_1 HTTP/1.1 Host: localhost:8080 { \u0026#34;id\u0026#34; : \u0026#34;ID_1\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34; : \u0026#34;2021-11-06T11:01:34.532+00:00\u0026#34; } Generate an OpenAPI file \u0026#34;Hey, this is all great, but in my team, we’re used to using Swagger. It’s a more comprehensive tool, we can even make requests directly from the documentation page!\u0026#34; — A developer with their own habits That’s an interesting remark! Moving away from Swagger means going from an interactive page to a completely static HTML page. So, would we lose out? Perhaps…​ but let’s not be hasty!\nLet’s get straight to the point: it’s entirely possible to generate a file describing the API in a \u0026#34;Swagger-compatible\u0026#34; format, i.e., the OpenAPI format. However, it’s not available by default. We need to add a small dependency that extends the capabilities of Spring REST Docs:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.epages\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;restdocs-api-spec-mockmvc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.14.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; And when documenting an endpoint, we now need to import the document() method from this dependency:\nimport static com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper.document; When running the tests again, a new \u0026#34;snippet\u0026#34; is generated: resource.json.\nNewly generated files preview It’s a snippet in the OpenAPI format. Now, all we have to do is gather these snippets into a single file, just like we already do for the AsciiDoc version. However, this time we don’t need a \u0026#34;root file\u0026#34;; we will use a plugin.\nBut…​ there’s a problem ahead! The added dependency only provides a Gradle plugin, and our project uses Maven! No need to panic, though. The community is vast and considerate, and there is a plugin for generating the final documentation: restdocs-spec-maven-plugin.\nBy adding it to the \u0026#34;build\u0026#34; section of the pom.xml file, we can generate an OpenAPI file in addition to the previous HTML file:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.github.berkleytechnologyservices\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;restdocs-spec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.21\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;generate\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;Demo Spring REST Docs\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;host\u0026gt;localhost:8080\u0026lt;/host\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.directory}/generated-docs\u0026lt;/outputDirectory\u0026gt; \u0026lt;filename\u0026gt;openapi\u0026lt;/filename\u0026gt; \u0026lt;specification\u0026gt;OPENAPI_V3\u0026lt;/specification\u0026gt; \u0026lt;description\u0026gt;API description for Demo Spring REST Docs service\u0026lt;/description\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Similar to the HTML file, since this openapi.yml file is located in an exposed directory, it will be accessible once the application is launched. From that point on, you are free to provide it as the entry point to an instance of Swagger UI.\nTo do that, you can quickly test the quality of the generated file by starting an instance of Swagger UI:\ndocker run -p 80:8080 swaggerapi/swagger-ui Once the container is up and running, simply go to http://localhost (assuming Swagger UI is running on port 80 based on our above command) and provide the address of the openapi.yml file in the search bar at the top of the page. The documentation will appear…​ with all the usual Swagger functionality. So, is the developer with their own habits satisfied?\nPreview of the documentation rendered by Swagger UI So, are you starting to be convinced by Spring REST Docs?\nIn any case, I am convinced, and I will continue to deploy it on the projects I have the pleasure of working on!\nUseful links:\nComparison of Spring REST Docs and OpenAPI by Baeldung\nMy demo project\n","permalink":"https://alainnicolas.fr/en/blog/make-documentation-great-again-bonus/","tags":["AsciiDoc","Spring REST Docs","Java","Spring Boot"],"title":"Make documentation great again (Bonus)"},{"categories":null,"contents":" We have previously seen a presentation of the AsciiDoc language and the benefits of \u0026#34;Documentation as Code\u0026#34;.\nHowever, it is possible to make the documentation writing process even easier! It is customary to document the API that we develop to facilitate its maintenance and interactions with its consumers.\nEven though one of the 4 values of agility promotes \u0026#34;working software over comprehensive documentation,\u0026#34; we will see that we can provide both at the same time with a particularly useful tool.\nSpring REST Docs Spring REST Docs greatly simplifies the process of writing API documentation by combining manual writing with the injection of auto-generated sections.\nIt relies on Spring MVC Test, widely used for testing the \u0026#34;web\u0026#34; layer of a Spring application. In other words, adding it to an already ongoing project does not require a complete overhaul of its tests…​ and that is obviously a good thing!\nA demonstration project In order to base our discussion on a concrete example, I propose a small demonstration project available on GitHub.\nIn such cases, there is no need to imagine a very complicated use case, so I have implemented a simple CRUD that manipulates a Company object. A Company is defined by an ID, a name, a location, and a creation date.\nTo resemble a real project, I have also added an interaction with a MongoDB database that needs to be started before launching the application. To add a Company, you can make a POST request with the following body:\n{ \u0026#34;name\u0026#34;: \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2021-10-29\u0026#34; } Add the necessary dependency First and foremost, you need to add the \u0026#34;Spring REST Docs\u0026#34; dependency to the project:\nExcerpt from the pom.xml file: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.restdocs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-restdocs-mockmvc\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; As for the other dependencies that make up this application, there is no need to list them here as they are very common and independent of the documentation generation tool.\nModify Controller tests to generate documentation Now we can look at the tests and add specific instructions to generate the elements we are interested in. Let’s focus on the test for the request to retrieve a Company based on its ID.\nNote that I have simply mocked the service layer for these controller tests, so they are clearly not end-to-end tests.\nExcerpt from CompanyControllerTest: @Test void getCompany() throws Exception { final String ID = \u0026#34;ID_1\u0026#34;; when(companyService.getCompany(ID)).thenReturn(company1); this.mockMvc.perform(RestDocumentationRequestBuilders.get(\u0026#34;/companies/{id}\u0026#34;, ID)) // (1) .andExpect(handler().handlerType(CompanyController.class)) // (2) .andExpect(handler().methodName(\u0026#34;getCompany\u0026#34;)) .andDo(print()) .andExpect(status().isOk()) .andExpect(content().json(objectMapper.writeValueAsString(company1))) .andDo(document( // (3) \u0026#34;getCompany\u0026#34;, ControllerTestUtils.preprocessRequest(), ControllerTestUtils.preprocessResponse(), pathParameters(parameterWithName(\u0026#34;id\u0026#34;).description(\u0026#34;The requested company id\u0026#34;)), // (4) responseFields( // (5) fieldWithPath(\u0026#34;id\u0026#34;).description(\u0026#34;The company unique ID\u0026#34;), fieldWithPath(\u0026#34;name\u0026#34;).description(\u0026#34;The company name\u0026#34;), fieldWithPath(\u0026#34;location\u0026#34;).description(\u0026#34;The company location\u0026#34;), fieldWithPath(\u0026#34;creationDate\u0026#34;).description(\u0026#34;The company creation date\u0026#34;)))); } First, we use RestDocumentationRequestBuilders to make the request instead of the traditional MockMvcRequestBuilders. The difference? It captures request information for documentation purposes.\nWe perform the usual tests: check that the request is handled by the getCompany() method of the CompanyController, that the response has HTTP status code 200, and that it contains the expected object.\nNew instruction! Here we trigger the generation of documentation, named \u0026#34;getCompany\u0026#34; for this request. Let’s forget about the next two lines for now.\nDocument the variable part of the URL: the Company ID is passed as a \u0026#34;path parameter\u0026#34;, and we can describe it (even though in our case this parameter is quite self-explanatory).\nDocument the response of the request: we can describe each field of the Company.\nFrom now on, if you add a field to the Company object without documenting it in this test, it will fail. The same goes if you remove a field, as the test will try to document a non-existing element. This provides a new level of safety: your API cannot evolve without you being fully aware of it!\nSo, we have a test that documents the most obvious passing case: when a Company corresponding to the requested ID is found. Let’s move on to a new test for the opposite case.\nExcerpt from CompanyControllerTest with an error test @Test void getCompanyNotFound() throws Exception { final String ID = \u0026#34;ID_3\u0026#34;; when(companyService.getCompany(ID)).thenThrow(new CompanyNotFoundException()); this.mockMvc.perform(RestDocumentationRequestBuilders.get(\u0026#34;/companies/{id}\u0026#34;, ID)) .andExpect(handler().handlerType(CompanyController.class)) .andExpect(handler().methodName(\u0026#34;getCompany\u0026#34;)) .andDo(print()) .andExpect(status().isNotFound()) .andDo(document( \u0026#34;getCompanyNotFound\u0026#34;, // (1) ControllerTestUtils.preprocessRequest(), ControllerTestUtils.preprocessResponse())); } Just like in the previous test, we add the document(…​) instruction, but this time with a new identifier (\u0026#34;getCompanyNotFound\u0026#34;) to differentiate the generated documentation in this new case.\nHowever, it is not necessary to generate additional documentation here, as the description of the \u0026#34;path parameter\u0026#34; has already been done in the previous test, and the request returns nothing but an HTTP 404 error.\nWhen running the two tests we just saw, files (referred to as \u0026#34;snippets\u0026#34;) will be generated under target/generated-snippets. And unsurprisingly, they are .adoc files!\nIf we open, for example, getCompany/response-fields.adoc, we can find:\n|=== |Path|Type|Description |`+id+` |`+String+` |The company unique ID |`+name+` |`+String+` |The company name |`+location+` |`+String+` |The company location |`+creationDate+` |`+String+` |The company creation date |=== Let’s be honest: even though we can see that the entered elements are present in the test, it is not very readable…​ And there are no less than 14 files generated for two small tests, who would read that?!\nIt is time to generate more readable documentation!\nThe documentation source file One file to rule them all. That’s the goal we should set ourselves to make our documentation viable.\nSo, we will add a file under source/asciidoctor: index.adoc. This is where we will inject our snippets (and only the ones we are interested in), with some manually added text to make the whole thing more digestible.\nExcerpt from the file index.adoc that will become the documentation === Get one company // (1) .Request \\include::{snippets}/getCompany/http-request.adoc[] // (2) .Path parameters \\include::{snippets}/getCompany/path-parameters.adoc[] // (3) .Response \\include::{snippets}/getCompany/http-response.adoc[] // (4) .Response fields \\include::{snippets}/getCompany/response-fields.adoc[] // (5) .Response if the company was not found \\include::{snippets}/getCompanyNotFound/http-response.adoc[] // (6) Let’s give a small title to this part of the documentation to describe the endpoint being documented below.\nWe inject a snippet containing the request, which is probably the best representation of it.\nWe inject a snippet containing the description of the \u0026#34;path parameter\u0026#34;.\nAnd the snippet illustrating the received response.\nDon’t forget the snippet describing the response fields.\nWe had two tests on this endpoint, so let’s show what the response looks like when the Company is not found.\nFor now, we have just created a documentation skeleton. Depending on your IDE, you may already see a rendering!\nTo improve the documentation, you can add a table of contents, a main title, etc. These are basic AsciiDoc features and annotations, and I suggest we don’t dwell on them. You can get an idea of what can be done by looking at the file index.adoc of the demonstration project.\nGenerating the documentation Now that we know how to generate snippets and how to combine them into a single file, it’s time to automate the generation of the HTML page that will contain all the documentation.\nFor that, we will rely on a Maven plugin: asciidoctor-maven-plugin.\nThe configuration to add to the file pom.xml \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.asciidoctor\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;asciidoctor-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-docs\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;process-asciidoc\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;backend\u0026gt;html\u0026lt;/backend\u0026gt; \u0026lt;doctype\u0026gt;book\u0026lt;/doctype\u0026gt; \u0026lt;attributes\u0026gt; \u0026lt;snippets\u0026gt;${project.build.directory}/generated-snippets\u0026lt;/snippets\u0026gt; \u0026lt;/attributes\u0026gt; \u0026lt;logHandler\u0026gt; \u0026lt;outputToConsole\u0026gt;true\u0026lt;/outputToConsole\u0026gt; \u0026lt;failIf\u0026gt; \u0026lt;severity\u0026gt;DEBUG\u0026lt;/severity\u0026gt; \u0026lt;/failIf\u0026gt; \u0026lt;/logHandler\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.restdocs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-restdocs-asciidoctor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-restdocs.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jacoco\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jacoco-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${jacoco-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;prepare-agent\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;prepare-agent\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-report\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;verify\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;report\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; This code generates an HTML page from the target/generated-snippets directory while blocking the build in case of errors (missing snippet, etc.).\nBy executing a command like mvn package, the tests will be run, the snippets will be generated, and finally the index.html page will be created from the index.adoc file. It can be found in target/generated-docs.\nPreview of the generated HTML page So, we have seen how to generate an HTML page describing the endpoints of a Spring Boot application using Spring REST Docs, without refactoring all controller tests.\nThe tool is quite comprehensive, and there are still many small details to cover to be more exhaustive. But I suggest we wait for an article presenting some bonus features!\nUseful links:\nSpring REST Docs Documentation\nA sample project provided by Spring\nMy demonstration project\n","permalink":"https://alainnicolas.fr/en/blog/make-documentation-great-again-part-2/","tags":["AsciiDoc","Spring REST Docs","Java","Spring Boot"],"title":"Make documentation great again (2/2)"},{"categories":null,"contents":" Do you also love the consent banners that are now found on (almost) every website and dream of having one on your own? It’s normal, it looks so much more professional…​ And it’s also about respecting the rules!\nGDPR, the real Cookie Monster What is a cookie? It is a small, fairly simple file that is placed in our browser when we visit certain websites. It is used, among other things, to identify us during our next visit. This can sometimes be quite useful: the website can anticipate our display preferences, for example.\nBut when an advertising network shared by multiple websites tracks our behavior from one site to another, for example, to offer us advertisements on Amazon related to our Facebook pages…​ the practical aspect fades behind the frightening aspect. Our personal data is no longer personal, even though we haven’t been asked for our consent.\nTo combat this trend, the GDPR requires obtaining the consent of users before processing their personal data.\nGDPR: a quick reminder The General Data Protection Regulation (GDPR) is a European Parliament regulation aimed at improving the protection of individuals\u0026#39; data, in particular by holding data processors accountable.\nIt also grants more powers to authorities such as the CNIL (French Data Protection Authority), which is responsible for enforcing European directives transposed into French law.\nThe consequence One direct consequence of this is quite visible when browsing the Internet: we are constantly asked for our consent regarding cookies.\nAnd often, it’s hard to ignore the question…​ Invasive banners, pop-ups in the foreground, all means are used to divert our attention from the information we want to access. By improving the privacy of our data, the GDPR has also significantly deteriorated our web browsing experience.\nThe GDPR is not limited to cookie consent, but the goal here is not to list all the ins and outs of an 80-page+ legal text.\nThe challenge of doing things properly But before blaming designers or developers who had no choice, let’s try to understand the difficulties involved in implementing user consent collection.\nListing cookies The first step is to list the cookies used by the website.\nOften, we find one (or several!) cookies for audience measurement, such as those deposited by Google Analytics or its competitors. But we also need to think about all the third-party services used…​\nFor example, on the Talan Labs blog that I mentioned in the creation here, we use Disqus to manage the comment system.\nWe must not forget the integration of video players like YouTube, which bring their own cookies, or the use of social media features, etc.\nMaking cookies optional As mentioned earlier, in order to comply with CNIL recommendations, the website must not deposit cookies without the visitor’s consent.\nThis means that we need to manage 3 states:\nNo consent: no cookies are deposited, and the user must not be tracked.\nPartial consent: the user agrees to certain cookies, and only these are deposited.\nFull consent: the user agrees to all cookies, and all cookies can be deposited.\nWe need to always know if consent has been given and be able to disable features if necessary. This inevitably has a direct impact on the source code of the relevant page…​\nAs a result, we end up with small pieces of modified code that look like this:\nif (isGoogleConsentGiven) { // Add Google Analytics } if (isDisqusConsentGiven) { // Add Disqus } Not (Too Much) Degrading the User Experience As mentioned earlier, even if cookies are refused (or at least some of them), the user should be able to continue navigating as normally as possible. Therefore, it’s essential to ensure that the core functionalities of our page are not impacted by the absence of a service.\nFor example, a page that lists YouTube videos as thumbnails…​ without YouTube…​ doesn’t look great. So, it’s important to provide a small message explaining the unusual appearance of the page.\nFurthermore, consent banners and overlays often add complexity and clutter to already busy web pages, sometimes significantly impacting the user’s browsing experience. Despite recent improvements due to CNIL’s reminders, it’s still challenging to consistently refuse cookies with prominently placed \u0026#34;ACCEPT\u0026#34; buttons.\nIn addition to the technical aspect, there are also UX considerations to avoid returning to the web of 20 years ago…​\nThe Solution: Tarteaucitron As we have seen, this process of compliance can be relatively long and complex. Fortunately, it’s time to introduce the tool that can greatly simplify your task: [Tarteaucitron](https://tarteaucitron.io)!\nWithout realizing it, you have probably already used Tarteaucitron, as it is used on many websites (over 20,000 claimed on the publisher’s site).\nIt is an incredibly versatile tool, offering a free open-source version and a paid version that provides continuous updates and a WordPress plugin. The JavaScript library is also available on [npm](https://www.npmjs.com/package/tarteaucitronjs), making it easy to install.\nTo integrate Tarteaucitron into your page, you need to add the following code:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;/tarteaucitron/tarteaucitron.js\u0026#34;/\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; tarteaucitron.init({ // Configuration }); \u0026lt;/script\u0026gt; The configuration covers all aspects of the tool: the position of the banner, display options for various choice buttons, etc. Modifying the appearance of the banner, icon, and choice screen may require some effort, but it’s not overly complicated.\nPreview of the banner on this site, at the bottom of the page Next, for each service, you can replace your current integration with Tarteaucitron’s. For example, if you used to add Google Analytics like this:\n\u0026lt;!-- Global site tag (gtag.js) - Google Analytics --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;UA-XXXXXXX\u0026#39;); \u0026lt;/script\u0026gt; You now need to replace that piece of code with:\n\u0026lt;!-- Cookies management for Google Analytics --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; tarteaucitron.user.gtagUa = \u0026#39;UA-XXXXXXX\u0026#39;; (tarteaucitron.job = tarteaucitron.job || []).push(\u0026#39;gtag\u0026#39;); \u0026lt;/script\u0026gt; At any time, the user can change their choices by clicking on the icon located at the bottom right of the screen, as seen on this site. They will then access the detailed choice screen:\nPreview of the choice screen Similarly, while navigating the site, if a functionality is disabled due to the absence of a cookie, Tarteaucitron will automatically prompt the user to accept the associated cookies if they want to use it:\nPreview of a disabled comment feature without consent for Disqus In summary, Tarteaucitron helps overcome the difficulties listed above:\nBy including over a hundred commonly used services by default, it covers the vast majority of cases on your site.\nIt manages consent on a service-by-service basis.\nIt avoids degrading the user experience by providing simple banners and cleanly replacing refused integrations.\nWith its easy installation and configuration, Tarteaucitron will undoubtedly continue to gain many users in the coming years, benefiting internet users everywhere!\n","permalink":"https://alainnicolas.fr/en/blog/easily-make-your-website-gdpr-compliant/","tags":["GDPR","Tarteaucitron","Blogging"],"title":"(Easily) Make your website GDPR compliant!"},{"categories":null,"contents":" Who likes writing documentation for their product or API? Who is tired of finding outdated documentation when arriving on a project? Are you a developer and feel concerned? It’s time to switch to \u0026#34;Documentation as Code\u0026#34;!\nIt is customary to document the API we develop, to facilitate its maintenance and interactions with its \u0026#34;consumers\u0026#34;. Even if one of the 4 values of agility promotes \u0026#34;working software over comprehensive documentation\u0026#34;, we will see that we can propose both at the same time thanks to a particularly practical tool.\n","permalink":"https://alainnicolas.fr/en/series/make-documentation-great-again/","tags":null,"title":"Make documentation great again"},{"categories":null,"contents":" Who enjoys writing documentation for their product or API? Who is tired of finding outdated documentation when joining a project?\nAre you a developer who can relate? It’s time to switch to \u0026#34;Documentation as Code\u0026#34;!\nThe documentation everyone knows The most basic example of \u0026#34;Documentation as Code\u0026#34; is the traditional README.md file at the root of every project. Ideally, this file contains the minimal documentation for the project, installation instructions, and perhaps a description of the main features.\nIn reality, this file is too often neglected and rarely kept up to date. And that’s a shame! Since it’s part of your project, it’s versioned, and there’s no need to open a new tool to change the documentation (we see you, Confluence or SharePoint!).\nAnd you’ve probably noticed that by default, it’s a .md file, which means it’s written in Markdown, a language that allows you to format text using tags (define a title, bold text, etc.).\nMarkdown logo The huge advantage: a human can read it without feeling like they’re reading code (useful for non-technical stakeholders of a project), but your IDE or GitHub can also display it as a nicely presented HTML page. Very convenient for making your documentation available to the public.\nA step forward: AsciiDoc But sometimes documentation goes beyond simple text, and we need to display tables, which are poorly handled in Markdown. Another disadvantage of Markdown: it doesn’t provide semantic meaning to the generated content, which means you have to manually insert HTML to benefit from CSS classes or specific tags.\nTo meet different emerging needs, \u0026#34;flavors\u0026#34; of Markdown have appeared. So many different versions, different interpreters…​ and in the end, we lost the promised universal language.\nEnter AsciiDoc. Like Markdown, it’s a language of tags that is quite simple and readable by a human even without interpretation. But it offers tags to display a much wider range of data types than basic Markdown.\nAsciidoctor logo Another advantage is the ability to include a file in an AsciiDoc text. For example, you can display code present in a project file and see that code snippet updated in the documentation every time the source file is modified.\nFor the more curious, Asciidoctor provides a detailed comparison between AsciiDoc and Markdown.\nAn example of a document generated with Asciidoctor As I mentioned last year in a post, I have the pleasure of supervising students in a series of courses on Blockchain technologies. In these courses, a significant emphasis is placed on practical work, and yes, we even talk about cheese…​\nExcerpt from a tutorial PDF The lab supports are regularly updated, including during the sessions when students notice errors or need more details. The process of making the updated version available would be much slower if the support were written in Word, for example, which would require manual export to PDF and then sending it by email, etc.\nIn our case, by requesting generation in both PDF and HTML formats for the support materials, and using a simple GitLab CI job, we can make the new version of the support available to the students in just a few moments.\nSame excerpt from a tutorial PDF, this time in HTML What if we did better? Okay, we’ve just seen that there is a tag language for formatting text for documentation purposes (even for writing books!), more advanced than Markdown…​ and certainly more relevant to the world of computers than LaTeX, which has other interests.\nHowever, we still need to write this documentation, and that doesn’t solve all our problems. What if I told you that we can go further? Find out more in the next article!\nUseful links:\nAsciiDoc language documentation\nSyntax quick reference\nAsciiDocLIVE, an online AsciiDoc editor\n","permalink":"https://alainnicolas.fr/en/blog/make-documentation-great-again-part-1/","tags":["AsciiDoc","Markdown"],"title":"Make documentation great again (1/2)"},{"categories":null,"contents":" As I have already mentioned in a [previous post](link:/en/blog/blockchain-training-engineers-of-tomorrow/), the partnership between Talan and EPF made me discover the role of a trainer. But we went further by proposing a long-term project to a group of 5th-year students.\nShared expectations Before diving into professional life through the end-of-studies internship, students must confront the business world through a 5-month project. Between the school project and the real project, it is an opportunity to put into practice the acquired knowledge and discover the \u0026#34;real\u0026#34; work methods of a company.\nPromo 2020 in their room @ Talan — 10/16/2019 Obviously, these pedagogical expectations interested us because it allows us to face the challenges raised by a blockchain project, which we don’t always have time to implement otherwise.\nFor this, we needed to move away from a purely academic project, in other words, a \u0026#34;disposable\u0026#34; one, and focus on a subject that would engage both our teams and the students.\nThe Share2Gether project Unlike the one-day creation of a voting system that we implemented during the initial training, we wanted a much more comprehensive and long-term project. And because using blockchain technology just for the sake of using it doesn’t make much sense, we needed to imagine a plausible use case.\nThis is how the Share2Gether project was born, aiming to offer an event organization solution, similar to what [Meetup.com](https://www.meetup.com/) provides. Indeed, when hosting different meetups in our premises, we have regularly encountered \u0026#34;no-shows,\u0026#34; which means registered participants who do not attend the event without bothering to free up their spot.\nBlockchain meetup @ Talan — 05/29/2019 While this behavior is known to event organizers, it is nonetheless frustrating, as it raises issues such as food waste when ordering a buffet or prevents people who genuinely want to attend from doing so, finding events falsely marked as full. All types of events are affected, including blockchain-oriented meetups.\nThrough a reputation system, including bonuses and penalties, we aim to encourage registered participants to attend or at least prioritize canceling their registration to provide the organizer with a clear count of participants.\nImagining an organization Starting from the product idea, we quickly imagine a technical implementation, the technologies to be used, etc. But it is necessary to convey these ideas to the students and, above all, make them want to take ownership of the project.\nConvinced of the values advocated by [Talan Labs](https://blog.talanlabs.com/) regarding work methods (solidarity, continuous improvement, etc.), it was natural to take advantage of this opportunity to transmit the principles of agility to the newly formed team.\nBut the method alone is not enough! During the first year of the project, we gradually discovered that certain fundamental concepts were not addressed in the students\u0026#39; academic curriculum. Design thinking, source code management, automated testing—there were many diverse topics that required targeted training during the project, mainly at the request of the students themselves.\nPromo 2020 in their classroom @ Talan — 10/16/2019 In order to anticipate these questions during the second year of the project, we proposed to the 2020 cohort of students to attend these training sessions from the beginning. Naturally, there is a significant gap between the theory of \u0026#34;writing tests is necessary\u0026#34; and its practical application during development. Nevertheless, the idea of the essential nature of tests was firmly ingrained from the outset.\nOnce the foundations were established, a routine could be established, punctuated by agile meetings. Every Wednesday, the students had a dedicated room reserved for them, but they could easily seek the assistance of Talan Labs employees present in the surrounding areas. This proximity also allowed them to catch a glimpse of the daily life of a developer and the dynamics of a project team.\nFrom struggle to mastery These two years of the project shared certain characteristics, starting with a challenging initial period for the students. New languages, new technologies, an unfamiliar work method…​ all of these were obstacles to overcome.\nPromo 2020 in their classroom @ Talan — 10/16/2019 This is undoubtedly the main value we wanted to convey: even lacking confidence and doubting the teachings of the school, the students are capable of working \u0026#34;for real\u0026#34; and creating value. At the end of the project, the students\u0026#39; feedback was unequivocal:\nThey were happy with the results achieved and almost surprised by how much they had accomplished.\nIn retrospect, we can say that we did not expect such feedback. We aimed to train students by providing them with new skills, but what made the most difference was the conveyed confidence. Before mastering new technologies like blockchain, the engineer of tomorrow must have confidence in their abilities (including their ability to learn and discover).\nA successful presentation…​ and it’s done! What would a school project be without a final presentation? We believed it was important to cultivate these presentation skills just as much as technical skills. That’s why we offered the students several \u0026#34;mock\u0026#34; presentations of their final presentation.\nPromo 2020 during a \u0026#39;mock\u0026#39; presentation @ Talan — 01/15/2020 First among ourselves, to identify the points expected by the school and the main messages to convey. Then, in front of a few colleagues who hadn’t followed the project in detail but could identify any overlooked messages, while the designers would provide advice on the presentation materials themselves.\nA new addition this year, as we had time at the end of the project: a presentation to Talan Directors. Our HR Director, Communications Director, and even the CEO of Talan Labs were able to attend the students\u0026#39; presentation. And what a presentation it was! If there’s one strength that an EPF generalist engineer has, it’s the ability to convey messages with conviction and clarity, sometimes even with humor.\nPromo 2020 during the final presentation @ Talan — 01/22/2020 With this experience, which they admitted was more stressful than usual presentations, the students were able to deliver a high-quality final presentation to their professors and supervisors. For two consecutive years, I had the pleasure of seeing the supervised groups succeed in their presentations and express genuine joy when talking about the work they had done.\nA promising future Between the training provided in the 4th and 5th years (discussed in a previous post) and the Share2Gether project, Talan Labs has made a significant investment, which is undoubtedly not without interest. By showcasing our skills and mindset, providing confidence and knowledge to the students, we also wanted to inspire them to join us.\nTherefore, it is with pleasure that we now count new EPF engineers among our ranks (from internships to permanent positions). And we don’t plan to stop there: the upcoming years will be just as eventful!\nFind the article presenting the Blockchain training provided at EPF since 2018 and all the articles on the #blockchain topic!\n","permalink":"https://alainnicolas.fr/en/blog/blockchain-from-a-school-project-to-the-business-world/","tags":["Blockchain","EPF","Formation"],"title":"Blockchain: From a school project to the business world"},{"categories":null,"contents":" For several years now, I have had the opportunity to teach students at EPF. Switching to the other side of teaching is not an easy task, especially without experience…​ The partnership with EPF has made me discover the role of trainer. We went further by mentoring a group of students on a project for a whole semester.\nFeedback on some extraordinary days!\n","permalink":"https://alainnicolas.fr/en/series/formation-blockchain-epf/","tags":null,"title":"Blockchain Training at EPF"},{"categories":null,"contents":" For the past two years, I have had the opportunity and pleasure to work with students from EPF, a general engineering school where I graduated in 2015. Transitioning to the teaching side of things is no easy task, especially without prior experience, but returning to \u0026#34;my\u0026#34; school has greatly facilitated this change in perspective.\nAn engineering school has the heavy responsibility of keeping its curriculum up to date to prepare students as effectively as possible for their entry into the workforce. That is why Talan has developed a partnership with EPF for a series of training programs on the theme of Blockchain for their 4th and 5th year students in the computer science field.\nConveying ideas Our first challenge was to create a theoretical training program. Working on this relatively young technology with disruptive applications brings strong convictions and ideas about what the blockchain can do, but also, and above all, what it cannot do. The implications of implementing such a distributed system have more impacts on organizations and individuals than the technology itself on information systems. Nevertheless, when facing a student audience, it is necessary to closely examine the technical components of the blockchain to grasp its relevance.\nThere are numerous ideas to convey, covering a wide range of themes, from the smallest technical details to change management, complex concepts, and governance. Formalizing and organizing these ideas into themes, articulating them logically and coherently, detailing the concepts without delving too deep into the implementation, and adapting to the assumed knowledge of students we do not know…​ These are all challenges to overcome, but not without interest. They are, in fact, the best way to realize which concepts we master and which ones deserve further attention.\nDefinition of a blockchain in 5 words Through numerous \u0026#34;dry run\u0026#34; presentations of the material to colleagues, we can identify remaining questions, areas where we rushed through complex points, and, above all, we can refine the interactive and illustrated nature of the discourse to keep it engaging.\nFrom trial by fire to polished presentation After the first edition of the training program under real conditions, with students who seemed interested and asked dozens of questions, we did not want to stop there. The objective was to transition from a basic training program to a comprehensive journey covering multiple Blockchain and DLT technologies for our clients.\nIt is clear that a training program on such a rapidly evolving subject must remain alive and evolving, even if it requires constant investment.\nTheoretical training @ EPF — 04/06/2019 That is why the students from the 2020 cohort benefited from a significantly revised training program, similar to the one we provide to our clients or during BBL (Brown Bag Lunch) sessions.\nTo avoid losing spontaneity and falling into a routine, I try to change at least one slide per presentation, always adapt the content to the audience or the duration, and even take different angles, such as at ESGI, where I also gave a conference on the risks of the blockchain.\nAdapting one’s discourse to the audience is essential: when facing a highly skilled and functional audience, we skim over the technical aspects, while with developers, it is often necessary to delve into the depths of the blockchain, discussing consensus algorithms, and so on.\nFrom theory to practice, there’s only the creation of a tutorial Without a direct request from EPF, we wanted to limit the time devoted to theoretical presentations and allow more time for practical work.\nIn order to remain relatively accessible and in line with the idea of true decentralization, we initially chose to focus on Ethereum, probably the most accessible blockchain.\nBut where to start?\nCryptoZombies Our first experiments with Ethereum date back to the end of 2016, before a fully developed ecosystem was in place. However, by keeping an eye on the latest developments, we couldn’t miss the resounding success of [CryptoZombies](https://cryptozombies.io/fr/), which is undoubtedly the best way to learn the Solidity language of Ethereum smart contracts.\nCryptoZombies was created by [Loom Network](https://loomx.io/) to make learning Solidity easier. Convinced of its relevance, we believe that our students should benefit from these excellent exercises. Beyond the convenience it provides for us, CryptoZombies is well thought out and particularly user-friendly. It would be a shame to create a dedicated training program when the community agrees to use it.\nBy creating an army of zombies, students acquire the technical basics of smart contract development. It is then time to move on to even more serious things!\nDecentralized Voting For the plunge into the deep end, we chose to develop a decentralized voting application as a practical work (TP).\nTo ensure that no one is left behind and to facilitate the supervision of this TP, we decided to create the most comprehensive support possible and provide a set of unit tests that validate each step of the development process.\nTP Support Cover Page This upfront preparation was far from insignificant and took several weeks:\nCreation of a minimal voting system (allowing the creation of elections with candidates and voting for one of them in each election) and validation of the developments through unit tests.\nDrafting of a first version of the support material, based on the recent developments that had taken place.\nCompletion of the TP by 10 of our colleagues, based solely on the support material, to gather as much feedback as possible.\nCorrection and improvement of the support material, as well as the exercises to be performed (finding the right balance between providing complete guidance and avoiding confusion in the instructions, establishing an exhaustive list of prerequisites, etc.).\nNew tests under quasi-real conditions, and so on.\nAnd then the day arrives. Around sixty students have to follow our instructions, starting with setting up their development environment. And that’s where the first difficulties arise.\nTP @ EPF — 04/06/2019 Obviously, on a developer’s PC, most of the prerequisites are already in place (mastered environment variables, installed and mastered tools, etc.). But on computers with varying performance and operating systems, with highly heterogeneous levels of mastery and sometimes overlooked instructions, the first barrage of questions is surprising. What seems obvious to us is not so for everyone else, and that is our first lesson. What seems complex to us is not necessarily so for everyone else either, and that is our best surprise.\nWhile we expected difficulties in understanding and writing smart contracts, for example, most of the issues encountered were actually related to calling them from a web page. These notions of smart contracts and their language (Solidity) may be relatively innovative, but for beginners, web development is equally new.\nFeedback We came to share knowledge but also to benefit from the lessons that the students have to offer. By creating an atmosphere of trust (which is easier when we come from the same school), it becomes possible to gather honest feedback and ideas for improvement from those closest to the field.\nDuring these training days, the balance between theory and practice seemed good, but the difficulties encountered in web development hindered the students\u0026#39; experience.\nThere is no doubt that we take these issues into account in order to address them in the next session. For example, by providing more unit tests that provide stricter guidance or a more satisfying and appealing web interface for everyone.\nWatch the video presentation of these training sessions for EPF students conducted by Talan:\nBeyond these occasional training sessions of a few days in the 4th and 5th years, Talan Labs has also embarked on supporting a smaller project group (5 in 2018/2019 and 8 in 2019/2020), always focusing on the blockchain theme.\nThis is the subject of another [article](https://alainncls.medium.com/blockchain-du-projet-scolaire-au-monde-de-lentreprise-1ba3de9bc255?source=friends_link\u0026amp;sk=7cace238c7a64debde0c5010760557ec)!\nThis article was originally published on [Medium](https://alainncls.medium.com/blockchain-former-les-ing%C3%A9nieurs-de-demain-e0044e9a2e8c?source=friends_link\u0026amp;sk=e750e36ac6c0411048cfad9179560be8).\n","permalink":"https://alainnicolas.fr/en/blog/blockchain-training-engineers-of-tomorrow/","tags":["Blockchain","EPF","Training"],"title":"Blockchain: Training the Engineers of Tomorrow"},{"categories":null,"contents":" Discovery of Corda, the DLT developed by consortium R3 that claims to be a blockchain. Right or wrong?\n","permalink":"https://alainnicolas.fr/en/series/corda/","tags":null,"title":"Corda"},{"categories":null,"contents":" The reports of the meetups hosted by Talan that I had the pleasure of welcoming.\n","permalink":"https://alainnicolas.fr/en/series/meetups-talan/","tags":null,"title":"Meetups at Talan"},{"categories":null,"contents":" First presented on October 4 at the 1st Parliamentary Forum of the Blockchain, the virtual currency of the Talan group is about to enter the production phase. An opportunity to present in detail the ins and outs of a project, and an initiative unlike any other: Talan Coin!\nTalan Coin Banner Blockchain, everyone talks about it, from mainstream media to the most innovative startups. And yet, few are the ideas that go as far as the production phase. That’s why Talan wanted to stand out and offer its employees a real application of this booming technology. This is how the idea of an internal currency was born from the desire to unite the staff of an international group around a common sharing model.\nTalan Coin, that’s its name, abbreviated as \u0026#39;TC\u0026#39;, is a currency based on the blockchain, and more particularly on Ethereum. Ethereum is a widely used blockchain, and its currency, Ether, is the second largest capitalization of cryptocurrencies, behind Bitcoin. The main contribution of Ethereum compared to Bitcoin is its ability to integrate unalterable logic and business rules since stored on the blockchain itself. These are the famous \u0026#34;Smart Contracts\u0026#34;, a concept that we will develop more fully in an upcoming article.\nWhy launch this currency? Like all companies, Talan seeks to catalyze collaboration within it. Also, Talan wants to strengthen its unity as a group despite distinct entities and a distribution over 4 continents and develop a horizontality far from the classic vertical hierarchies. An internal currency, common to all employees, regardless of their location or hierarchical role, allows to find a common point between all the actors of the company. It is also a way to allow everyone to better exchange, including between distinct teams, thus asserting a horizontal hierarchy.\nOverview of the Talan Coin tracking Dashboard Besides, by monitoring the evolution of the exchanges of this currency and the reasons for transfer, Talan Coin allows immediate feedback of trends at Talan. The introduction of a gamification component also encourages even more use, through the organization of challenges.\nWhat does Talan Coin look like? Overview of the mobile version of Talan Coin Talan Coin is accessible to all group employees through a mobile application (iOS and Android) downloadable from the official stores of Google and Apple. The mobile orientation of Talan Coin guarantees a fairly wide adoption of the application and allows everyone to access the system. This eliminates any potential connection restrictions for our employees on assignment with our clients or simply on the move.\nOverview of the web version of Talan Coin Also, to further extend the reach and accessibility of the product, a web version, desktop-oriented is being finalized, to allow a new experience, always.\nAnd concretely, how does it work? There are two types of wallets in circulation: personal wallets for all employees and team wallets, managed by one or more person(s) identified as leader(s) on a subject or a team.\nPersonal wallets can exchange money with all other personal wallets, but not directly with team wallets. The latter allow to \u0026#39;reward\u0026#39; employees working for the benefit of a team.\nHow to earn Talan Coins? How to earn TC? As we have seen previously, the main objective of Talan Coin is to promote exchanges and sharing within the company. Thus, all actions aimed at helping Talan and more broadly facilitating daily life are likely to be rewarded in Talan Coins.\nThe identified axes for the moment are as follows:\nHelping colleagues (occasional helping hand, technical support, etc.)\nInnovate (launching a new project, an idea that can change internal processes, etc.)\nGet involved in group life (participation in communities, organization of meetups, writing articles for Talan blogs, etc.)\nBeing nice (the morning joke, spontaneous help, etc.)\nSharing knowledge (production of training, organization of a technical presentation, mentoring and sponsorship, etc.)\nStreamlining internal processes (participation in a call for tenders, cooptation, etc.)\nHow to give Talan Coins? How to give TC? The transfer feature is available from mobile and web applications. A reason is associated with each transfer, to provide usage indicators, but also to leaving a trace in everyone’s history.\nThese transfer reasons also allow to create as many ranking categories in the \u0026#39;gamification\u0026#39; section. It is planned to evolve this list according to the needs that will be discovered.\nHow to convert your Talan Coins? How to convert your TC? Active participation in the life of the company allows you to earn Talan Coins, but you still need to be able to use them, among other things through a conversion into goods or services. This is how Talan relies on 2 shops of its own to provide products or services payable in Talan Coin.\nTalan Campus Historically, this is the first \u0026#39;shop\u0026#39; identified as a source of offers for Talan Coin. Indeed, the Talan Campus already hosts all the internal and external training delivered to Talan employees. It therefore seemed natural to add new training offers not previously available, to open new opportunities.\nIt is not a question of making what was free since the launch of the Talan Campus pay, but rather to extend the reach of a central tool to the Talan group.\nOverview of the Talan Campus Each team leader has the possibility to add an offer in the Campus, specifying a price in TC, which makes the offer visible from the mobile and web applications. One can therefore imagine an employee wishing to attend a paid training course not previously covered by Talan and who would make a request to a team leader. The latter would then only have to buy a place at this training in € (via the classic process), and make it available on the Talan Campus with an associated TC price. The TCs collected by the sale of this offer to the employee would therefore return to the team wallet, reimbursing the € investment initiated.\nSuch a device also guarantees that training offers are no longer negotiated on a case-by-case basis, but open to all, ensuring greater equity and making offers available to all employees.\nTalan Coin Shop To provide offers in addition to training, we have developed a new shop suitable for selling objects and services. This \u0026#39;Talan Coin Shop\u0026#39; allows the buyer and seller to be notified of the successful completion of a transaction.\nOverview of the Talan Coin Shop Here too, team leaders can add offers, composed of a title, a description, an image and a price. When making a purchase, the seller and buyer are notified by email once the transaction has been successfully completed.\nTalan Coin is a melting currency Attention melting currency! To stimulate the economy of Talan Coin, we have imagined using a principle of melting currency. The concept is simple: when an employee does not spend the money he has received after a month, he loses a percentage of the sum earned. Without penalizing an employee who would be on leave for example, the sum deducted will thus be re-injected into the global system, while motivating everyone to use, exchange or spend their Talan Coins.\nIndeed, for us, one of the key indicators of the success of the Talan Coin project simply lies in the number of transactions that will take place on a daily basis. This would thus indicate the adoption of the currency in the daily life of the company.\nWhat’s next… The realization of such a project, resolutely innovative, has not been without stumbling blocks, nor without discoveries and teachings of all kinds. We want to share our experience and new knowledge with as many people as possible, if only in view of the help provided by the increasingly important community of Ethereum developers.\n\u0026gt;\u0026gt;\u0026gt; The official Talan Coin website \u0026lt;\u0026lt;\u0026lt;\n","permalink":"https://alainnicolas.fr/en/blog/what-is-talancoin/","tags":["Blockchain","Ethereum"],"title":"What is Talan Coin?"},{"categories":null,"contents":" Perhaps for you the summer 2016 series was Stranger Things, but in the vast world of Java, a completely different series with multiple twists and turns has attracted all attention. Acquisition, rebellion, and polls, dive into the revolution of Java EE!\n","permalink":"https://alainnicolas.fr/en/series/java-ee-new-direction/","tags":null,"title":"Java EE: new direction"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/android/","tags":null,"title":"Android"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/angular/","tags":null,"title":"Angular"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/asciidoc/","tags":null,"title":"AsciiDoc"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/asseth/","tags":null,"title":"Asseth"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/azure-devops/","tags":null,"title":"Azure DevOps"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/bitcoin/","tags":null,"title":"Bitcoin"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/blockchain/","tags":null,"title":"Blockchain"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/blogging/","tags":null,"title":"Blogging"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/corda/","tags":null,"title":"Corda"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/development/","tags":null,"title":"Development"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/dlt/","tags":null,"title":"DLT"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/docker/","tags":null,"title":"Docker"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/epf/","tags":null,"title":"EPF"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/ethereum/","tags":null,"title":"Ethereum"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/ganache/","tags":null,"title":"Ganache"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/rgpd/","tags":null,"title":"GDPR"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/git/","tags":null,"title":"Git"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/github/","tags":null,"title":"GitHub"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/gitlab/","tags":null,"title":"GitLab"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/google/","tags":null,"title":"Google"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/gradle/","tags":null,"title":"Gradle"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/hardhat/","tags":null,"title":"Hardhat"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/hugo/","tags":null,"title":"Hugo"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/hyperledger-fabric/","tags":null,"title":"Hyperledger Fabric"},{"categories":null,"contents":"Insights.io is a competitor to Google Analytics that stands out for being much less invasive regarding the privacy of visitors to the sites it is used on.\nBy not depositing any cookies, it collects less information while still retaining what really matters: which pages are visited, for how long, etc. This tool allows us to do away with the \u0026ldquo;cookie consent\u0026rdquo; banners that often detract from our web experience.\nTo gather this information, Insights.io relies on a JavaScript library published by the creators of the service. I took the liberty of creating a fork of it that filters out robots and other visitors that we do not want to include in the final statistics.\n","permalink":"https://alainnicolas.fr/en/tags/insights/","tags":null,"title":"Insights"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/intellij-idea/","tags":null,"title":"IntelliJ IDEA"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/ios/","tags":null,"title":"iOS"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/java/","tags":null,"title":"Java"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/java-swing/","tags":null,"title":"Java Swing"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/javascript/","tags":null,"title":"JavaScript"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/jira/","tags":null,"title":"Jira"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/jquery/","tags":null,"title":"jQuery"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/jsp/","tags":null,"title":"JSP"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/kanban/","tags":null,"title":"Kanban"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/linea/","tags":null,"title":"Linea"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/markdown/","tags":null,"title":"Markdown"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/maven/","tags":null,"title":"Maven"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/meetup/","tags":null,"title":"Meetup"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/microsoft/","tags":null,"title":"Microsoft"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/mongodb/","tags":null,"title":"MongoDB"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/mysql/","tags":null,"title":"mySQL"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/nestjs/","tags":null,"title":"NestJS"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/nodejs/","tags":null,"title":"Node.js"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/npm/","tags":null,"title":"NPM"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/oracledb/","tags":null,"title":"OracleDB"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/pnpm/","tags":null,"title":"pnpm"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/postgresql/","tags":null,"title":"PostgreSQL"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/quorum/","tags":null,"title":"Quorum"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/r3/","tags":null,"title":"R3"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/react/","tags":null,"title":"React"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/scrum/","tags":null,"title":"Scrum"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/security/","tags":null,"title":"Security"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/perso/","tags":null,"title":"Side project"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/solidity/","tags":null,"title":"Solidity"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/sonarqube/","tags":null,"title":"SonarQube"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/spring-boot/","tags":null,"title":"Spring Boot"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/spring-mvc/","tags":null,"title":"Spring MVC"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/spring-rest-docs/","tags":null,"title":"Spring REST Docs"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/tarteaucitron/","tags":null,"title":"Tarteaucitron"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/formation/","tags":null,"title":"Training"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/truffle/","tags":null,"title":"Truffle"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/typescript/","tags":null,"title":"TypeScript"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/vault/","tags":null,"title":"Vault"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/verax/","tags":null,"title":"Verax"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/vuejs/","tags":null,"title":"Vue.js"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/cycle-en-v/","tags":null,"title":"Waterfall"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/windows/","tags":null,"title":"Windows"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/wordpress/","tags":null,"title":"WordPress"},{"categories":null,"contents":"","permalink":"https://alainnicolas.fr/en/tags/yarn/","tags":null,"title":"Yarn"}]